# -*- coding: utf-8 -*-
"""NetworkScience_FinalProject.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TFABqXivV3aL1rpdXP-afZOVXRvdAYKf
"""

!pip install python-igraph
!pip install cairocffi
!pip install scikit-network
!pip install infomap
!pip install scipy
!pip install tabulate
!pip install powerlaw

import networkx as nx
import matplotlib.pyplot as plt
import numpy as np

import pandas as pd
import itertools
import igraph as ig
import ast
import cairocffi as cairo
import random
import math
import scipy.sparse as sp_sparse
import infomap
import json
import matplotlib.pyplot as plt
from networkx.algorithms.community import greedy_modularity_communities
from sknetwork.clustering import Louvain, get_modularity
from sknetwork.linalg import normalize
from sknetwork.utils import get_membership
from sknetwork.visualization import visualize_graph, visualize_bigraph
from sklearn.cluster import SpectralClustering
from sklearn.metrics.pairwise import pairwise_distances
from IPython.display import SVG
from sklearn.linear_model import LinearRegression
from sklearn import metrics
from infomap import Infomap
from scipy.stats import lognorm
from collections import defaultdict
from scipy.sparse import csgraph
from scipy.sparse.linalg import eigsh
from collections import Counter
import scipy
from random import randint
from tabulate import tabulate
from matplotlib import style
from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score
from scipy.stats import ttest_ind

G = nx.DiGraph()
edge_list_file = "congress.edgelist"
# Read the edge list file and add edges to the graph
with open(edge_list_file, 'r') as file:
    for line in file:
        parts = line.split()
        u = int(parts[0])
        v = int(parts[1])
        weight = float(parts[3][:-1])
        G.add_edge(u, v, weight=weight)

f = open('congress_network_data.json')
data = json.load(f)

inList = data[0]['inList']
inWeight = data[0]['inWeight']
outList = data[0]['outList']
outWeight = data[0]['outWeight']
usernameList = data[0]['usernameList']

G1 = G.copy()
mapping = {}
for i in range(0, len(usernameList)):
  mapping[i] = usernameList[i]
# Rename the nodes with the usernames
G1 = nx.relabel_nodes(G1, mapping)

party_affiliations = [
    "Democratic",  # SenatorBaldwin
    "Republican",  # SenJohnBarrasso
    "Democratic",  # SenatorBennet
    "Republican",  # MarshaBlackburn
    "Democratic",  # SenBlumenthal
    "Republican",  # RoyBlunt
    "Democratic",  # CoryBooker
    "Republican",  # JohnBoozman
    "Republican",  # SenatorBraun
    "Democratic",  # SenSherrodBrown
    "Democratic",  # SenatorCantwell
    "Republican",  # SenCapito
    "Democratic",  # SenatorCardin
    "Democratic",  # SenatorCarper
    "Democratic",  # SenBobCasey
    "Republican",  # SenBillCassidy
    "Democratic",  # ChrisCoons
    "Republican",  # JohnCornyn
    "Democratic",  # SenCortezMasto
    "Republican",  # SenTomCotton
    "Republican",  # SenKevinCramer
    "Republican",  # MikeCrapo
    "Republican",  # SenTedCruz
    "Republican",  # SteveDaines
    "Democratic",  # SenDuckworth
    "Democratic",  # SenatorDurbin
    "Republican",  # SenJoniErnst
    "Democratic",  # SenFeinstein
    "Republican",  # SenatorFischer
    "Democratic",  # SenGillibrand
    "Republican",  # LindseyGrahamSC
    "Republican",  # ChuckGrassley
    "Republican",  # SenatorHagerty
    "Democratic",  # SenatorHassan
    "Republican",  # HawleyMO
    "Democratic",  # MartinHeinrich
    "Democratic",  # SenatorHick
    "Democratic",  # maziehirono
    "Republican",  # SenJohnHoeven
    "Republican",  # SenHydeSmith
    "Republican",  # JimInhofe
    "Republican",  # SenRonJohnson
    "Democratic",  # timkaine
    "Democratic",  # SenMarkKelly
    "Republican",  # SenJohnKennedy
    "Independent",  # SenAngusKing
    "Democratic",  # SenAmyKlobuchar
    "Republican",  # SenatorLankford
    "Democratic",  # SenatorLeahy
    "Republican",  # SenMikeLee
    "Democratic",  # SenatorLujan
    "Republican",  # SenLummis
    "Democratic",  # Sen_JoeManchin
    "Democratic",  # SenMarkey
    "Democratic",  # SenatorMenendez
    "Democratic",  # SenJeffMerkley
    "Republican",  # JerryMoran
    "Republican",  # lisamurkowski
    "Democratic",  # ChrisMurphyCT
    "Democratic",  # PattyMurray
    "Democratic",  # ossoff
    "Democratic",  # SenAlexPadilla
    "Republican",  # RandPaul
    "Democratic",  # SenGaryPeters
    "Republican",  # senrobportman
    "Democratic",  # SenJackReed
    "Republican",  # SenatorRisch
    "Republican",  # SenatorRomney
    "Democratic",  # SenJackyRosen
    "Republican",  # marcorubio
    "Independent",  # SenSanders
    "Democratic",  # SenSchumer
    "Republican",  # SenRickScott
    "Republican",  # SenatorTimScott
    "Democratic",  # SenatorShaheen
    "Independent",  # SenatorSinema
    "Democratic",  # SenTinaSmith
    "Democratic",  # SenStabenow
    "Republican",  # SenDanSullivan
    "Democratic",  # SenatorTester
    "Republican",  # SenJohnThune
    "Republican",  # SenThomTillis
    "Republican",  # SenToomey
    "Republican",  # SenTuberville
    "Democratic",  # ChrisVanHollen
    "Democratic",  # MarkWarner
    "Democratic",  # SenatorWarnock
    "Democratic",  # SenWarren
    "Democratic",  # SenWhitehouse
    "Republican",  # SenatorWicker
    "Democratic",  # RonWyden
    "Republican",  # SenToddYoung
    "Democratic",  # RepAdams
    "Republican",  # Robert_Aderholt
    "Democratic",  # RepPeteAguilar
    "Republican",  # RepRickAllen
    "Democratic",  # RepColinAllred
    "Republican",  # RepArmstrongND
    "Republican",  # RepArrington
    "Democratic",  # RepAuchincloss
    "Democratic",  # RepCindyAxne
    "Republican",  # RepBrianBabin
    "Republican",  # RepDonBacon
    "Republican",  # RepJimBaird
    "Republican",  # RepBalderson
    "Republican",  # RepJimBanks
    "Republican",  # RepAndyBarr
    "Democratic",  # RepBarragan
    "Democratic",  # RepKarenBass
    "Democratic",  # RepBeatty
    "Democratic",  # RepBera
    "Democratic",  # RepDonBeyer
    "Republican",  # RepBice
    "Republican",  # RepAndyBiggsAZ
    "Republican",  # RepGusBilirakis
    "Democratic",  # SanfordBishop
    "Republican",  # RepDanBishop
    "Democratic",  # RepLBR
    "Republican",  # RepBoebert
    "Democratic",  # RepBonamici
    "Republican",  # RepBost
    "Democratic",  # RepBourdeaux
    "Democratic",  # RepBowman
    "Democratic",  # CongBoyle
    "Republican",  # RepKevinBrady
    "Republican",  # RepMoBrooks
    "Democratic",  # RepAnthonyBrown
    "Democratic",  # RepShontelBrown
    "Democratic",  # RepBrownley
    "Republican",  # VernBuchanan
    "Republican",  # RepKenBuck
    "Republican",  # RepLarryBucshon
    "Republican",  # RepTedBudd
    "Republican",  # RepTimBurchett
    "Republican",  # michaelcburgess
    "Democratic",  # RepCori
    "Democratic",  # RepCheri
    "Republican",  # RepKatCammack
    "Democratic",  # RepCarbajal
    "Democratic",  # RepCardenas
    "Republican",  # RepMikeCarey
    "Republican",  # RepJerryCarl
    "Democratic",  # RepAndreCarson
    "Republican",  # RepBuddyCarter
    "Republican",  # JudgeCarter
    "Democratic",  # RepTroyCarter
    "Democratic",  # RepEdCase
    "Democratic",  # RepCasten
    "Democratic",  # USRepKCastor
    "Democratic",  # JoaquinCastrotx
    "Republican",  # RepCawthorn
    "Republican",  # RepSteveChabot
    "Republican",  # RepLizCheney
    "Democratic",  # CongresswomanSC
    "Democratic",  # RepJudyChu
    "Democratic",  # RepKClark
    "Democratic",  # RepYvetteClarke
    "Democratic",  # repcleaver
    "Republican",  # RepBenCline
    "Republican",  # RepCloudTX
    "Democratic",  # WhipClyburn
    "Republican",  # Rep_Clyde
    "Democratic",  # RepCohen
    "Republican",  # TomColeOK04
    "Republican",  # RepJamesComer
    "Democratic",  # GerryConnolly
    "Democratic",  # RepLouCorrea
    "Democratic",  # RepJimCosta
    "Democratic",  # RepJoeCourtney
    "Democratic",  # RepAngieCraig
    "Democratic",  # RepCharlieCrist
    "Democratic",  # RepJasonCrow
    "Republican",  # RepJohnCurtis
    "Democratic",  # RepDavids
    "Republican",  # WarrenDavidson
    "Republican",  # RodneyDavis
    "Democratic",  # RepDean
    "Democratic",  # RepPeterDeFazio
    "Democratic",  # RepDianaDeGette
    "Democratic",  # rosadelauro
    "Democratic",  # RepDelBene
    "Democratic",  # repdelgado
    "Democratic",  # RepValDemings
    "Democratic",  # RepDeSaulnier
    "Democratic",  # RepTedDeutch
    "Republican",  # MarioDB
    "Democratic",  # RepDebDingell
    "Democratic",  # RepLloydDoggett
    "Republican",  # RepDonaldsPress
    "Democratic",  # USRepMikeDoyle
    "Republican",  # RepJeffDuncan
    "Republican",  # DrNealDunnFL2
    "Republican",  # RepTomEmmer
    "Democratic",  # RepEscobar
    "Democratic",  # RepAnnaEshoo
    "Democratic",  # RepEspaillat
    "Republican",  # RepRonEstes
    "Democratic",  # RepDwightEvans
    "Republican",  # RepPatFallon
    "Republican",  # RepFeenstra
    "Republican",  # RepDrewFerguson
    "Republican",  # RepFischbach
    "Democratic",  # RepBrianFitz
    "Republican",  # RepChuck
    "Democratic",  # RepFletcher
    "Democratic",  # RepBillFoster
    "Republican",  # virginiafoxx
    "Democratic",  # RepLoisFrankel
    "Republican",  # RepFranklin
    "Republican",  # RepRussFulcher
    "Republican",  # RepMattGaetz
    "Republican",  # RepGallagher
    "Democratic",  # RepRubenGallego
    "Democratic",  # RepGaramendi
    "Republican",  # RepGarbarino
    "Democratic",  # RepChuyGarcia
    "Republican",  # RepMikeGarcia
    "Democratic",  # RepSylviaGarcia
    "Republican",  # RepBobGibbs
    "Democratic",  # RepCarlos
    "Republican",  # replouiegohmert
    "Democratic",  # RepGolden
    "Democratic",  # RepJimmyGomez
    "Republican",  # RepTonyGonzales
    "Republican",  # RepJenniffer
    "Democratic",  # RepGonzalez
    "Republican",  # RepBobGood
    "Republican",  # Lancegooden
    "Republican",  # RepGosar
    "Democratic",  # RepJoshG
    "Republican",  # RepKayGranger
    "Republican",  # RepGarretGraves
    "Democratic",  # RepAlGreen
    "Republican",  # RepMarkGreen
    "Republican",  # RepMTG
    "Republican",  # RepMGriffith
    "Democratic",  # RepRaulGrijalva
    "Republican",  # RepGrothman
    "Republican",  # RepMichaelGuest
    "Republican",  # RepGuthrie
    "Democratic",  # RepJoshHarder
    "Republican",  # RepHarshbarger
    "Republican",  # RepHartzler
    "Democratic",  # RepJahanaHayes
    "Republican",  # repkevinhern
    "Republican",  # RepHerrell
    "Republican",  # CongressmanHice
    "Democratic",  # RepBrianHiggins
    "Republican",  # RepClayHiggins
    "Republican",  # RepFrenchHill
    "Democratic",  # jahimes
    "Republican",  # RepAshleyHinson
    "Democratic",  # RepHorsford
    "Democratic",  # RepHoulahan
    "Democratic",  # LeaderHoyer
    "Republican",  # RepRichHudson
    "Democratic",  # RepHuffman
    "Republican",  # RepHuizenga
    "Republican",  # repdarrellissa
    "Democratic",  # JacksonLeeTX18
    "Republican",  # RepRonnyJackson
    "Republican",  # RepJacobs
    "Democratic",  # RepSaraJacobs
    "Democratic",  # RepJayapal
    "Democratic",  # RepJeffries
    "Republican",  # RepBillJohnson
    "Republican",  # RepDustyJohnson
    "Democratic",  # RepEBJ
    "Democratic",  # RepHankJohnson
    "Republican",  # RepMikeJohnson
    "Democratic",  # RepMondaire
    "Republican",  # Jim_Jordan
    "Republican",  # RepDaveJoyce
    "Republican",  # RepJohnJoyce
    "Republican",  # RepJohnKatko
    "Democratic",  # USRepKeating
    "Republican",  # RepFredKeller
    "Republican",  # MikeKellyPA
    "Democratic",  # RepRobinKelly
    "Democratic",  # RepRoKhanna
    "Democratic",  # RepDanKildee
    "Democratic",  # RepDerekKilmer
    "Democratic",  # RepAndyKimNJ
    "Republican",  # RepYoungKim
    "Democratic",  # RepRonKind
    "Democratic",  # RepKirkpatrick
    "Democratic",  # CongressmanRaja
    "Democratic",  # RepAnnieKuster
    "Republican",  # RepDavidKustoff
    "Republican",  # RepLaHood
    "Republican",  # RepLaMalfa
    "Democratic",  # JimLangevin
    "Democratic",  # RepRickLarsen
    "Democratic",  # RepJohnLarson
    "Republican",  # boblatta
    "Republican",  # RepLaTurner
    "Democratic",  # RepLawrence
    "Democratic",  # RepAlLawsonJr
    "Democratic",  # RepBarbaraLee
    "Democratic",  # RepSusieLee
    "Democratic",  # RepTeresaLF
    "Republican",  # RepDLesko
    "Republican",  # RepJuliaLetlow
    "Democratic",  # RepAndyLevin
    "Democratic",  # RepMikeLevin
    "Democratic",  # RepTedLieu
    "Republican",  # USRepLong
    "Republican",  # RepLoudermilk
    "Democratic",  # RepLowenthal
    "Republican",  # RepFrankLucas
    "Republican",  # RepBlaine
    "Democratic",  # RepElaineLuria
    "Republican",  # RepNancyMace
    "Democratic",  # RepMalinowski
    "Republican",  # RepMalliotakis
    "Democratic",  # RepMaloney
    "Democratic",  # RepSeanMaloney
    "Democratic",  # RepKManning
    "Republican",  # RepThomasMassie
    "Republican",  # RepBrianMast
    "Democratic",  # DorisMatsui
    "Democratic",  # RepLucyMcBath
    "Republican",  # GOPLeader
    "Republican",  # RepMcCaul
    "Republican",  # RepLisaMcClain
    "Democratic",  # BettyMcCollum04
    "Democratic",  # RepMcEachin
    "Democratic",  # RepMcGovern
    "Republican",  # PatrickMcHenry
    "Republican",  # RepMcKinley
    "Democratic",  # RepGregoryMeeks
    "Republican",  # RepMeijer
    "Democratic",  # RepGraceMeng
    "Republican",  # RepMeuser
    "Democratic",  # RepKweisiMfume
    "Republican",  # RepMMM
    "Republican",  # RepCarolMiller
    "Republican",  # RepMaryMiller
    "Republican",  # RepAlexMooney
    "Republican",  # RepBarryMoore
    "Republican",  # RepBlakeMoore
    "Democratic",  # RepGwenMoore
    "Democratic",  # RepJoeMorelle
    "Republican",  # RepMullin
    "Republican",  # RepGregMurphy
    "Democratic",  # RepStephMurphy
    "Democratic",  # RepJerryNadler
    "Democratic",  # gracenapolitano
    "Democratic",  # RepRichardNeal
    "Democratic",  # RepJoeNeguse
    "Republican",  # RepTroyNehls
    "Republican",  # RepNewhouse
    "Democratic",  # RepMarieNewman
    "Democratic",  # DonaldNorcross
    "Republican",  # RepRalphNorman
    "Democratic",  # EleanorNorton
    "Democratic",  # RepOHalleran
    "Republican",  # JayObernolte
    "Democratic",  # Ilhan
    "Republican",  # RepBurgessOwens
    "Republican",  # CongPalazzo
    "Democratic",  # FrankPallone
    "Republican",  # USRepGaryPalmer
    "Democratic",  # RepJimmyPanetta
    "Democratic",  # RepChrisPappas
    "Democratic",  # BillPascrell
    "Democratic",  # RepDonaldPayne
    "Democratic",  # SpeakerPelosi
    "Democratic",  # RepPerlmutter
    "Democratic",  # RepScottPeters
    "Republican",  # RepPfluger
    "Democratic",  # RepDeanPhillips
    "Democratic",  # chelliepingree
    "Democratic",  # StaceyPlaskett
    "Democratic",  # repmarkpocan
    "Democratic",  # RepKatiePorter
    "Democratic",  # RepPressley
    "Democratic",  # RepDavidEPrice
    "Democratic",  # RepMikeQuigley
    "Democratic",  # RepRaskin
    "Republican",  # GReschenthaler
    "Democratic",  # RepKathleenRice
    "Republican",  # RepTomRice
    "Republican",  # cathymcmorris
    "Republican",  # RepMikeRogersAL
    "Republican",  # RepJohnRose
    "Republican",  # RepRosendale
    "Democratic",  # RepDeborahRoss
    "Republican",  # RepDavidRouzer
    "Republican",  # RepChipRoy
    "Democratic",  # RepRoybalAllard
    "Democratic",  # RepRaulRuizMD
    "Democratic",  # Call_Me_Dutch
    "Democratic",  # RepBobbyRush
    "Democratic",  # RepTimRyan
    "Democratic",  # Kilili_Sablan
    "Republican",  # RepMariaSalazar
    "Democratic",  # RepLindaSanchez
    "Democratic",  # RepSarbanes
    "Republican",  # SteveScalise
    "Democratic",  # RepMGS
    "Democratic",  # janschakowsky
    "Democratic",  # RepAdamSchiff
    "Democratic",  # RepSchneider
    "Democratic",  # RepSchrader
    "Democratic",  # RepKimSchrier
    "Democratic",  # RepDavid
    "Republican",  # AustinScottGA08
    "Democratic",  # BobbyScott
    "Republican",  # PeteSessions
    "Democratic",  # RepTerriSewell
    "Democratic",  # BradSherman
    "Democratic",  # RepSherrill
    "Republican",  # CongMikeSimpson
    "Democratic",  # RepSires
    "Democratic",  # RepSlotkin
    "Democratic",  # RepAdamSmith
    "Republican",  # RepAdrianSmith
    "Republican",  # RepJasonSmith
    "Republican",  # RepSmucker
    "Democratic",  # RepDarrenSoto
    "Democratic",  # RepSpanberger
    "Republican",  # RepSpartz
    "Democratic",  # RepSpeier
    "Democratic",  # Rep_Stansbury
    "Democratic",  # RepGregStanton
    "Republican",  # RepPeteStauber
    "Republican",  # RepSteel
    "Republican",  # RepStefanik
    "Republican",  # RepBryanSteil
    "Republican",  # RepGregSteube
    "Democratic",  # RepHaleyStevens
    "Republican",  # RepChrisStewart
    "Democratic",  # RepStricklandWA
    "Democratic",  # RepTomSuozzi
    "Democratic",  # RepSwalwell
    "Democratic",  # RepMarkTakano
    "Republican",  # claudiatenney
    "Democratic",  # BennieGThompson
    "Republican",  # RepThompson
    "Republican",  # RepTiffany
    "Republican",  # RepTimmons
    "Democratic",  # repdinatitus
    "Democratic",  # RepRashida
    "Democratic",  # RepPaulTonko
    "Democratic",  # NormaJTorres
    "Democratic",  # RepRitchie
    "Democratic",  # RepLoriTrahan
    "Democratic",  # RepDavidTrone
    "Republican",  # RepMikeTurner
    "Democratic",  # RepUnderwood
    "Republican",  # RepDavidValado
    "Republican",  # RepBethVanDuyne
    "Democratic",  # RepJuanVargas
    "Democratic",  # RepVeasey
    "Democratic",  # NydiaVelazquez
    "Republican",  # RepAnnWagner
    "Republican",  # RepWalberg
    "Republican",  # RepWalorski
    "Republican",  # michaelgwaltz
    "Democratic",  # RepDWStweets
    "Democratic",  # RepBonnie
    "Republican",  # RepWebster
    "Democratic",  # PeterWelch
    "Republican",  # RepWesterman
    "Democratic",  # RepWexton
    "Democratic",  # RepSusanWild
    "Democratic",  # RepNikema
    "Republican",  # RepRWilliams
    "Democratic",  # RepWilson
    "Republican",  # RepJoeWilson
    "Republican",  # RobWittman
    "Republican",  # rep_stevewomack
    "Democratic",  # RepJohnYarmuth
    "Republican"   # RepLeeZeldin
]

party_affiliations_dict = {
    "SenatorBaldwin": "Democratic",
    "SenJohnBarrasso": "Republican",
    "SenatorBennet": "Democratic",
    "MarshaBlackburn": "Republican",
    "SenBlumenthal": "Democratic",
    "RoyBlunt": "Republican",
    "CoryBooker": "Democratic",
    "JohnBoozman": "Republican",
    "SenatorBraun": "Republican",
    "SenSherrodBrown": "Democratic",
    "SenatorCantwell": "Democratic",
    "SenCapito": "Republican",
    "SenatorCardin": "Democratic",
    "SenatorCarper": "Democratic",
    "SenBobCasey": "Democratic",
    "SenBillCassidy": "Republican",
    "ChrisCoons": "Democratic",
    "JohnCornyn": "Republican",
    "SenCortezMasto": "Democratic",
    "SenTomCotton": "Republican",
    "SenKevinCramer": "Republican",
    "MikeCrapo": "Republican",
    "SenTedCruz": "Republican",
    "SteveDaines": "Republican",
    "SenDuckworth": "Democratic",
    "SenatorDurbin": "Democratic",
    "SenJoniErnst": "Republican",
    "SenFeinstein": "Democratic",
    "SenatorFischer": "Republican",
    "SenGillibrand": "Democratic",
    "LindseyGrahamSC": "Republican",
    "ChuckGrassley": "Republican",
    "SenatorHagerty": "Republican",
    "SenatorHassan": "Democratic",
    "HawleyMO": "Republican",
    "MartinHeinrich": "Democratic",
    "SenatorHick": "Democratic",
    "maziehirono": "Democratic",
    "SenJohnHoeven": "Republican",
    "SenHydeSmith": "Republican",
    "JimInhofe": "Republican",
    "SenRonJohnson": "Republican",
    "timkaine": "Democratic",
    "SenMarkKelly": "Democratic",
    "SenJohnKennedy": "Republican",
    "SenAngusKing": "Independent",
    "SenAmyKlobuchar": "Democratic",
    "SenatorLankford": "Republican",
    "SenatorLeahy": "Democratic",
    "SenMikeLee": "Republican",
    "SenatorLujan": "Democratic",
    "SenLummis": "Republican",
    "Sen_JoeManchin": "Democratic",
    "SenMarkey": "Democratic",
    "SenatorMenendez": "Democratic",
    "SenJeffMerkley": "Democratic",
    "JerryMoran": "Republican",
    "lisamurkowski": "Republican",
    "ChrisMurphyCT": "Democratic",
    "PattyMurray": "Democratic",
    "ossoff": "Democratic",
    "SenAlexPadilla": "Democratic",
    "RandPaul": "Republican",
    "SenGaryPeters": "Democratic",
    "senrobportman": "Republican",
    "SenJackReed": "Democratic",
    "SenatorRisch": "Republican",
    "SenatorRomney": "Republican",
    "SenJackyRosen": "Democratic",
    "marcorubio": "Republican",
    "SenSanders": "Independent",
    "SenSchumer": "Democratic",
    "SenRickScott": "Republican",
    "SenatorTimScott": "Republican",
    "SenatorShaheen": "Democratic",
    "SenatorSinema": "Independent",
    "SenTinaSmith": "Democratic",
    "SenStabenow": "Democratic",
    "SenDanSullivan": "Republican",
    "SenatorTester": "Democratic",
    "SenJohnThune": "Republican",
    "SenThomTillis": "Republican",
    "SenToomey": "Republican",
    "SenTuberville": "Republican",
    "ChrisVanHollen": "Democratic",
    "MarkWarner": "Democratic",
    "SenatorWarnock": "Democratic",
    "SenWarren": "Democratic",
    "SenWhitehouse": "Democratic",
    "SenatorWicker": "Republican",
    "RonWyden": "Democratic",
    "SenToddYoung": "Republican",
    "RepAdams": "Democratic",
    "Robert_Aderholt": "Republican",
    "RepPeteAguilar": "Democratic",
    "RepRickAllen": "Republican",
    "RepColinAllred": "Democratic",
    "RepArmstrongND": "Republican",
    "RepArrington": "Republican",
    "RepAuchincloss": "Democratic",
    "RepCindyAxne": "Democratic",
    "RepBrianBabin": "Republican",
    "RepDonBacon": "Republican",
    "RepJimBaird": "Republican",
    "RepBalderson": "Republican",
    "RepJimBanks": "Republican",
    "RepAndyBarr": "Republican",
    "RepBarragan": "Democratic",
    "RepKarenBass": "Democratic",
    "RepBeatty": "Democratic",
    "RepBera": "Democratic",
    "RepDonBeyer": "Democratic",
    "RepBice": "Republican",
    "RepAndyBiggsAZ": "Republican",
    "RepGusBilirakis": "Republican",
    "SanfordBishop": "Democratic",
    "RepDanBishop": "Republican",
    "RepLBR": "Democratic",
    "RepBoebert": "Republican",
    "RepBonamici": "Democratic",
    "RepBost": "Republican",
    "RepBourdeaux": "Democratic",
    "RepBowman": "Democratic",
    "CongBoyle": "Democratic",
    "RepKevinBrady": "Republican",
    "RepMoBrooks": "Republican",
    "RepAnthonyBrown": "Democratic",
    "RepShontelBrown": "Democratic",
    "RepBrownley": "Democratic",
    "VernBuchanan": "Republican",
    "RepKenBuck": "Republican",
    "RepLarryBucshon": "Republican",
    "RepTedBudd": "Republican",
    "RepTimBurchett": "Republican",
    "michaelcburgess": "Republican",
    "RepCori": "Democratic",
    "RepCheri": "Democratic",
    "RepKatCammack": "Republican",
    "RepCarbajal": "Democratic",
    "RepCardenas": "Democratic",
    "RepMikeCarey": "Republican",
    "RepJerryCarl": "Republican",
    "RepAndreCarson": "Democratic",
    "RepBuddyCarter": "Republican",
    "JudgeCarter": "Republican",
    "RepTroyCarter": "Democratic",
    "RepEdCase": "Democratic",
    "RepCasten": "Democratic",
    "USRepKCastor": "Democratic",
    "JoaquinCastrotx": "Democratic",
    "RepCawthorn": "Republican",
    "RepSteveChabot": "Republican",
    "RepLizCheney": "Republican",
    "CongresswomanSC": "Democratic",
    "RepJudyChu": "Democratic",
    "RepKClark": "Democratic",
    "RepYvetteClarke": "Democratic",
    "repcleaver": "Democratic",
    "RepBenCline": "Republican",
    "RepCloudTX": "Republican",
    "WhipClyburn": "Democratic",
    "Rep_Clyde": "Republican",
    "RepCohen": "Democratic",
    "TomColeOK04": "Republican",
    "RepJamesComer": "Republican",
    "GerryConnolly": "Democratic",
    "RepLouCorrea": "Democratic",
    "RepJimCosta": "Democratic",
    "RepJoeCourtney": "Democratic",
    "RepAngieCraig": "Democratic",
    "RepCharlieCrist": "Democratic",
    "RepJasonCrow": "Democratic",
    "RepJohnCurtis": "Republican",
    "RepDavids": "Democratic",
    "WarrenDavidson": "Republican",
    "RodneyDavis": "Republican",
    "RepDean": "Democratic",
    "RepPeterDeFazio": "Democratic",
    "RepDianaDeGette": "Democratic",
    "rosadelauro": "Democratic",
    "RepDelBene": "Democratic",
    "repdelgado": "Democratic",
    "RepValDemings": "Democratic",
    "RepDeSaulnier": "Democratic",
    "RepTedDeutch": "Democratic",
    "MarioDB": "Republican",
    "RepDebDingell": "Democratic",
    "RepLloydDoggett": "Democratic",
    "RepDonaldsPress": "Republican",
    "USRepMikeDoyle": "Democratic",
    "RepJeffDuncan": "Republican",
    "DrNealDunnFL2": "Republican",
    "RepTomEmmer": "Republican",
    "RepEscobar": "Democratic",
    "RepAnnaEshoo": "Democratic",
    "RepEspaillat": "Democratic",
    "RepRonEstes": "Republican",
    "RepDwightEvans": "Democratic",
    "RepPatFallon": "Republican",
    "RepFeenstra": "Republican",
    "RepDrewFerguson": "Republican",
    "RepFischbach": "Republican",
    "RepBrianFitz": "Democratic",
    "RepChuck": "Republican",
    "RepFletcher": "Democratic",
    "RepBillFoster": "Democratic",
    "virginiafoxx": "Republican",
    "RepLoisFrankel": "Democratic",
    "RepFranklin": "Republican",
    "RepRussFulcher": "Republican",
    "RepMattGaetz": "Republican",
    "RepGallagher": "Republican",
    "RepRubenGallego": "Democratic",
    "RepGaramendi": "Democratic",
    "RepGarbarino": "Republican",
    "RepChuyGarcia": "Democratic",
    "RepMikeGarcia": "Republican",
    "RepSylviaGarcia": "Democratic",
    "RepBobGibbs": "Republican",
    "RepCarlos": "Democratic",
    "replouiegohmert": "Republican",
    "RepGolden": "Democratic",
    "RepJimmyGomez": "Democratic",
    "RepTonyGonzales": "Republican",
    "RepJenniffer": "Republican",
    "RepGonzalez": "Democratic",
    "RepBobGood": "Republican",
    "Lancegooden": "Republican",
    "RepGosar": "Republican",
    "RepJoshG": "Democratic",
    "RepKayGranger": "Republican",
    "RepGarretGraves": "Republican",
    "RepAlGreen": "Democratic",
    "RepMarkGreen": "Republican",
    "RepMTG": "Republican",
    "RepMGriffith": "Republican",
    "RepRaulGrijalva": "Democratic",
    "RepGrothman": "Republican",
    "RepMichaelGuest": "Republican",
    "RepGuthrie": "Republican",
    "RepJoshHarder": "Democratic",
    "RepHarshbarger": "Republican",
    "RepHartzler": "Republican",
    "RepJahanaHayes": "Democratic",
    "repkevinhern": "Republican",
    "RepHerrell": "Republican",
    "CongressmanHice": "Republican",
    "RepBrianHiggins": "Democratic",
    "RepClayHiggins": "Republican",
    "RepFrenchHill": "Republican",
    "jahimes": "Democratic",
    "RepAshleyHinson": "Republican",
    "RepHorsford": "Democratic",
    "RepHoulahan": "Democratic",
    "LeaderHoyer": "Democratic",
    "RepRichHudson": "Republican",
    "RepHuffman": "Democratic",
    "RepHuizenga": "Republican",
    "repdarrellissa": "Republican",
    "JacksonLeeTX18": "Democratic",
    "RepRonnyJackson": "Republican",
    "RepJacobs": "Republican",
    "RepSaraJacobs": "Democratic",
    "RepJayapal": "Democratic",
    "RepJeffries": "Democratic",
    "RepBillJohnson": "Republican",
    "RepDustyJohnson": "Republican",
    "RepEBJ": "Democratic",
    "RepHankJohnson": "Democratic",
    "RepMikeJohnson": "Republican",
    "RepMondaire": "Democratic",
    "Jim_Jordan": "Republican",
    "RepDaveJoyce": "Republican",
    "RepJohnJoyce": "Republican",
    "RepJohnKatko": "Republican",
    "USRepKeating": "Democratic",
    "RepFredKeller": "Republican",
    "MikeKellyPA": "Republican",
    "RepRobinKelly": "Democratic",
    "RepRoKhanna": "Democratic",
    "RepDanKildee": "Democratic",
    "RepDerekKilmer": "Democratic",
    "RepAndyKimNJ": "Democratic",
    "RepYoungKim": "Republican",
    "RepRonKind": "Democratic",
    "RepKirkpatrick": "Democratic",
    "CongressmanRaja": "Democratic",
    "RepAnnieKuster": "Democratic",
    "RepDavidKustoff": "Republican",
    "RepLaHood": "Republican",
    "RepLaMalfa": "Republican",
    "JimLangevin": "Democratic",
    "RepRickLarsen": "Democratic",
    "RepJohnLarson": "Democratic",
    "boblatta": "Republican",
    "RepLaTurner": "Republican",
    "RepLawrence": "Democratic",
    "RepAlLawsonJr": "Democratic",
    "RepBarbaraLee": "Democratic",
    "RepSusieLee": "Democratic",
    "RepTeresaLF": "Democratic",
    "RepDLesko": "Republican",
    "RepJuliaLetlow": "Republican",
    "RepAndyLevin": "Democratic",
    "RepMikeLevin": "Democratic",
    "RepTedLieu": "Democratic",
    "USRepLong": "Republican",
    "RepLoudermilk": "Republican",
    "RepLowenthal": "Democratic",
    "RepFrankLucas": "Republican",
    "RepBlaine": "Republican",
    "RepElaineLuria": "Democratic",
    "RepNancyMace": "Republican",
    "RepMalinowski": "Democratic",
    "RepMalliotakis": "Republican",
    "RepMaloney": "Democratic",
    "RepSeanMaloney": "Democratic",
    "RepKManning": "Democratic",
    "RepThomasMassie": "Republican",
    "RepBrianMast": "Republican",
    "DorisMatsui": "Democratic",
    "RepLucyMcBath": "Democratic",
    "GOPLeader": "Republican",
    "RepMcCaul": "Republican",
    "RepLisaMcClain": "Republican",
    "BettyMcCollum04": "Democratic",
    "RepMcEachin": "Democratic",
    "RepMcGovern": "Democratic",
    "PatrickMcHenry": "Republican",
    "RepMcKinley": "Republican",
    "RepGregoryMeeks": "Democratic",
    "RepMeijer": "Republican",
    "RepGraceMeng": "Democratic",
    "RepMeuser": "Republican",
    "RepKweisiMfume": "Democratic",
    "RepMMM": "Republican",
    "RepCarolMiller": "Republican",
    "RepMaryMiller": "Republican",
    "RepAlexMooney": "Republican",
    "RepBarryMoore": "Republican",
    "RepBlakeMoore": "Republican",
    "RepGwenMoore": "Democratic",
    "RepJoeMorelle": "Democratic",
    "RepMullin": "Republican",
    "RepGregMurphy": "Republican",
    "RepStephMurphy": "Democratic",
    "RepJerryNadler": "Democratic",
    "gracenapolitano": "Democratic",
    "RepRichardNeal": "Democratic",
    "RepJoeNeguse": "Democratic",
    "RepTroyNehls": "Republican",
    "RepNewhouse": "Republican",
    "RepMarieNewman": "Democratic",
    "DonaldNorcross": "Democratic",
    "RepRalphNorman": "Republican",
    "EleanorNorton": "Democratic",
    "RepOHalleran": "Democratic",
    "JayObernolte": "Republican",
    "Ilhan": "Democratic",
    "RepBurgessOwens": "Republican",
    "CongPalazzo": "Republican",
    "FrankPallone": "Democratic",
    "USRepGaryPalmer": "Republican",
    "RepJimmyPanetta": "Democratic",
    "RepChrisPappas": "Democratic",
    "BillPascrell": "Democratic",
    "RepDonaldPayne": "Democratic",
    "SpeakerPelosi": "Democratic",
    "RepPerlmutter": "Democratic",
    "RepScottPeters": "Democratic",
    "RepPfluger": "Republican",
    "RepDeanPhillips": "Democratic",
    "chelliepingree": "Democratic",
    "StaceyPlaskett": "Democratic",
    "repmarkpocan": "Democratic",
    "RepKatiePorter": "Democratic",
    "RepPressley": "Democratic",
    "RepDavidEPrice": "Democratic",
    "RepMikeQuigley": "Democratic",
    "RepRaskin": "Democratic",
    "GReschenthaler": "Republican",
    "RepKathleenRice": "Democratic",
    "RepTomRice": "Republican",
    "cathymcmorris": "Republican",
    "RepMikeRogersAL": "Republican",
    "RepJohnRose": "Republican",
    "RepRosendale": "Republican",
    "RepDeborahRoss": "Democratic",
    "RepDavidRouzer": "Republican",
    "RepChipRoy": "Republican",
    "RepRoybalAllard": "Democratic",
    "RepRaulRuizMD": "Democratic",
    "Call_Me_Dutch": "Democratic",
    "RepBobbyRush": "Democratic",
    "RepTimRyan": "Democratic",
    "Kilili_Sablan": "Democratic",
    "RepMariaSalazar": "Republican",
    "RepLindaSanchez": "Democratic",
    "RepSarbanes": "Democratic",
    "SteveScalise": "Republican",
    "RepMGS": "Democratic",
    "janschakowsky": "Democratic",
    "RepAdamSchiff": "Democratic",
    "RepSchneider": "Democratic",
    "RepSchrader": "Democratic",
    "RepKimSchrier": "Democratic",
    "RepDavid": "Democratic",
    "AustinScottGA08": "Republican",
    "BobbyScott": "Democratic",
    "PeteSessions": "Republican",
    "RepTerriSewell": "Democratic",
    "BradSherman": "Democratic",
    "RepSherrill": "Democratic",
    "CongMikeSimpson": "Republican",
    "RepSires": "Democratic",
    "RepSlotkin": "Democratic",
    "RepAdamSmith": "Democratic",
    "RepAdrianSmith": "Republican",
    "RepJasonSmith": "Republican",
    "RepSmucker": "Republican",
    "RepDarrenSoto": "Democratic",
    "RepSpanberger": "Democratic",
    "RepSpartz": "Republican",
    "RepSpeier": "Democratic",
    "Rep_Stansbury": "Democratic",
    "RepGregStanton": "Democratic",
    "RepPeteStauber": "Republican",
    "RepSteel": "Republican",
    "RepStefanik": "Republican",
    "RepBryanSteil": "Republican",
    "RepGregSteube": "Republican",
    "RepHaleyStevens": "Democratic",
    "RepChrisStewart": "Republican",
    "RepStricklandWA": "Democratic",
    "RepTomSuozzi": "Democratic",
    "RepSwalwell": "Democratic",
    "RepMarkTakano": "Democratic",
    "claudiatenney": "Republican",
    "BennieGThompson": "Democratic",
    "RepThompson": "Republican",
    "RepTiffany": "Republican",
    "RepTimmons": "Republican",
    "repdinatitus": "Democratic",
    "RepRashida": "Democratic",
    "RepPaulTonko": "Democratic",
    "NormaJTorres": "Democratic",
    "RepRitchie": "Democratic",
    "RepLoriTrahan": "Democratic",
    "RepDavidTrone": "Democratic",
    "RepMikeTurner": "Republican",
    "RepUnderwood": "Democratic",
    "RepDavidValado": "Republican",
    "RepBethVanDuyne": "Republican",
    "RepJuanVargas": "Democratic",
    "RepVeasey": "Democratic",
    "NydiaVelazquez": "Democratic",
    "RepAnnWagner": "Republican",
    "RepWalberg": "Republican",
    "RepWalorski": "Republican",
    "michaelgwaltz": "Republican",
    "RepDWStweets": "Democratic",
    "RepBonnie": "Democratic",
    "RepWebster": "Republican",
    "PeterWelch": "Democratic",
    "RepWesterman": "Republican",
    "RepWexton": "Democratic",
    "RepSusanWild": "Democratic",
    "RepNikema": "Democratic",
    "RepRWilliams": "Republican",
    "RepWilson": "Democratic",
    "RepJoeWilson": "Republican",
    "RobWittman": "Republican",
    "rep_stevewomack": "Republican",
    "RepJohnYarmuth": "Democratic",
    "RepLeeZeldin": "Republican"}

A = sp_sparse.csr_matrix(nx.adjacency_matrix(G))

def transition_matrix(G):
    adj_matrix = nx.to_numpy_array(G)
    row_sums = adj_matrix.sum(axis=1, keepdims=True)
    row_sums[row_sums == 0] = 1  # Avoid division by zero
    return adj_matrix / row_sums

# Function to calculate the stationary distribution of the Markov chain
def stationary_distribution(P):
    eigvals, eigvecs = np.linalg.eig(P.T)
    eigvec = eigvecs[:, np.isclose(eigvals, 1)]
    pi = eigvec / eigvec.sum()
    return pi.real.flatten()

# Function to calculate module visit frequencies
def module_frequencies(pi, community_assignment):
    module_visits = defaultdict(float)
    for node, freq in enumerate(pi):
        community = community_assignment[node]
        module_visits[community] += freq
    return module_visits

# Function to calculate entropy
def entropy(prob_dist):
    return -sum(p * np.log2(p) for p in prob_dist if p > 0)

# Function to implement the Map Equation for directed graphs
def map_equation(G, community_assignment):
    P = transition_matrix(G)
    pi = stationary_distribution(P)
    module_visits = module_frequencies(pi, community_assignment)

    # Entropy of node visit frequencies
    node_entropy = 0.0
    for i in range(len(G)):
        node_entropy += pi[i] * entropy(P[i])

    # Entropy of module visit frequencies
    module_entropy = entropy(module_visits.values())

    # Total description length
    q = len(set(community_assignment))  # Number of modules
    return q * module_entropy + node_entropy

"""# **Network analysis**"""

# Compute in-degree distribution
in_degree_sequence = sorted([d for n, d in G.in_degree()], reverse=True)
unique_in_degrees, in_degree_counts = np.unique(in_degree_sequence, return_counts=True)
in_degree_probability = in_degree_counts / len(in_degree_sequence)
in_cumulative_distribution = 1 - np.cumsum(in_degree_probability)

# Compute out-degree distribution
out_degree_sequence = sorted([d for n, d in G.out_degree()], reverse=True)
unique_out_degrees, out_degree_counts = np.unique(out_degree_sequence, return_counts=True)
out_degree_probability = out_degree_counts / len(out_degree_sequence)
out_cumulative_distribution = 1 - np.cumsum(out_degree_probability)

"""**Degree distribution**"""

# Compute the degree sequence
degree_sequence = sorted([d for n, d in G.degree()], reverse=True)

# Compute the frequency of each degree
degree_counts = np.bincount(degree_sequence)

# Exclude degrees with zero frequency
degree_bins = np.nonzero(degree_counts)[0]
frequency = degree_counts[degree_bins]

# Fit a linear regression line to the log-log plot of degree vs. frequency
log_degree = np.log(degree_bins).reshape(-1, 1)
log_frequency = np.log(frequency)
regression_model = LinearRegression().fit(log_degree, log_frequency)

# Get the slope and intercept of the linear regression line
slope = regression_model.coef_[0]
intercept = regression_model.intercept_

# Plot the log-log plot of degree vs. frequency
plt.figure(figsize=(8, 6))  # Adjust the figure size as needed
plt.loglog(degree_bins, frequency, 'o', markersize=5, alpha=0.6, label='Degree Distribution', color='b')
plt.plot(np.exp(log_degree), np.exp(intercept + slope * log_degree), 'r--', linewidth=2, label='Linear Regression Line')

# Add labels and title
plt.xlabel('Degree', size=12)
plt.ylabel('Frequency', size=12)
plt.title('Log-log Degree Distribution', size=14)

# Add grid
plt.grid(True, which='both', ls='--', linewidth=0.5, alpha=0.6)

# Add legend
plt.legend(fontsize='medium')

# Show plot
plt.tight_layout()
plt.show()

# Print the slope of the linear regression line (log-log)
print("Slope of the linear regression line (log-log):", slope)

degree_sequence = sorted([d for n, d in G.degree()], reverse=True)

# Compute the frequency of each degree
degree_counts = np.bincount(degree_sequence)

# Exclude degrees with zero frequency
degree_bins = np.nonzero(degree_counts)[0]
frequency = degree_counts[degree_bins]

# Identify the degree with the highest probability
k_max_probability = degree_bins[np.argmax(frequency)]
print("threshold:",k_max_probability)
# Exclude points below the threshold
threshold = k_max_probability
filtered_degree_bins = degree_bins[degree_bins >= threshold]
filtered_frequency = frequency[degree_bins >= threshold]

# Fit a linear regression line to the log-log plot of degree vs. frequency
log_degree = np.log(filtered_degree_bins).reshape(-1, 1)
log_frequency = np.log(filtered_frequency)
regression_model = LinearRegression().fit(log_degree, log_frequency)

# Get the slope and intercept of the linear regression line
slope = regression_model.coef_[0]
intercept = regression_model.intercept_

# Plot the log-log plot of degree vs. frequency
plt.figure(figsize=(8, 6))  # Adjust the figure size as needed
plt.loglog(degree_bins, frequency, 'o', markersize=5, alpha=0.6, label='Degree Distribution', color='b')
plt.plot(np.exp(log_degree), np.exp(intercept + slope * log_degree), 'r--', linewidth=2, label='Linear Regression Line')

# Add labels and title
plt.xlabel('Degree', size=12)
plt.ylabel('Frequency', size=12)
plt.title('Log-log Degree Distribution', size=14)

# Add grid
plt.grid(True, which='both', ls='--', linewidth=0.5, alpha=0.6)

# Add legend
plt.legend(fontsize='medium')

# Show plot
plt.tight_layout()
plt.show()

# Print the slope of the linear regression line (log-log)
print("Slope of the linear regression line (log-log):", slope)

# Compute in-degree distribution
in_degree_sequence = sorted([d for n, d in G.in_degree()], reverse=True)
unique_in_degrees, in_degree_counts = np.unique(in_degree_sequence, return_counts=True)
in_degree_probability = in_degree_counts / len(in_degree_sequence)
in_cumulative_distribution = 1 - np.cumsum(in_degree_probability)

# Compute out-degree distribution
out_degree_sequence = sorted([d for n, d in G.out_degree()], reverse=True)
unique_out_degrees, out_degree_counts = np.unique(out_degree_sequence, return_counts=True)
out_degree_probability = out_degree_counts / len(out_degree_sequence)
out_cumulative_distribution = 1 - np.cumsum(out_degree_probability)

# Create subplots
fig, axs = plt.subplots(1, 2, figsize=(12, 6))

# Plot in-degree distribution
axs[0].loglog(unique_in_degrees, in_degree_probability, 'bo', label="In-degree")
axs[0].set_title("In-Degree Distribution for usa_congress_network", size=16)
axs[0].set_xlabel("In-Degree (k)", size=12)
axs[0].set_ylabel("Probability (Pk)", size=12)
axs[0].legend()

# Plot out-degree distribution
axs[1].loglog(unique_out_degrees, out_degree_probability, 'ro', label="Out-degree")
axs[1].set_title("Out-Degree Distribution for usa_congress_network", size=16)
axs[1].set_xlabel("Out-Degree (k)", size=12)
axs[1].set_ylabel("Probability (Pk)", size=12)
axs[1].legend()

# Show the plots
plt.tight_layout()
plt.show()

# Assuming you have already computed in_degree_sequence, out_degree_sequence, and initialized G

# Compute in-degree distribution
unique_in_degrees, in_degree_counts = np.unique(in_degree_sequence, return_counts=True)
in_degree_probability = in_degree_counts / len(in_degree_sequence)

# Compute out-degree distribution
unique_out_degrees, out_degree_counts = np.unique(out_degree_sequence, return_counts=True)
out_degree_probability = out_degree_counts / len(out_degree_sequence)

# Create subplots
fig, axs = plt.subplots(1, 2, figsize=(12, 6))

# Plot in-degree distribution
axs[0].loglog(unique_in_degrees, in_degree_probability, 'bo', markersize=5, alpha=0.6, label="In-degree")

# Customize in-degree subplot
axs[0].set_title("In-Degree Distribution", size=16)
axs[0].set_xlabel("In-Degree (k)", size=12)
axs[0].set_ylabel("Probability (Pk)", size=12)
axs[0].legend()
axs[0].grid(True, which="both", ls="--", linewidth=0.5, alpha=0.6)

# Plot out-degree distribution
axs[1].loglog(unique_out_degrees, out_degree_probability, 'ro', markersize=5, alpha=0.6, label="Out-degree")

# Customize out-degree subplot
axs[1].set_title("Out-Degree Distribution", size=16)
axs[1].set_xlabel("Out-Degree (k)", size=12)
axs[1].set_ylabel("Probability (Pk)", size=12)
axs[1].legend()
axs[1].grid(True, which="both", ls="--", linewidth=0.5, alpha=0.6)

# Adjust layout and show plot
plt.tight_layout()
plt.show()

# Function to compute and fit power-law distribution
def fit_power_law(degree_sequence):
    unique_degrees, degree_counts = np.unique(degree_sequence, return_counts=True)
    degree_probability = degree_counts / len(degree_sequence)

    # Identify the degree with the highest probability
    k_max_probability = unique_degrees[np.argmax(degree_probability)]

    # Exclude points below the threshold
    threshold = k_max_probability
    filtered_degrees = unique_degrees[unique_degrees >= threshold]
    filtered_probability = degree_probability[unique_degrees >= threshold]

    # Fit a linear regression line to the log-log plot of degree vs. probability
    log_degree = np.log(filtered_degrees).reshape(-1, 1)
    log_probability = np.log(filtered_probability)
    regression_model = LinearRegression().fit(log_degree, log_probability)

    # Get the slope of the linear regression line
    slope = regression_model.coef_[0]
    intercept = regression_model.intercept_

    return unique_degrees, degree_probability, filtered_degrees, intercept, slope, threshold

in_degree_sequence = sorted([d for n, d in G.in_degree()], reverse=True)
out_degree_sequence = sorted([d for n, d in G.out_degree()], reverse=True)



# Compute and fit power-law for in-degree
in_degrees, in_degree_probability, filtered_in_degrees, in_intercept, in_slope, in_threshold = fit_power_law(in_degree_sequence)

# Compute and fit power-law for out-degree
out_degrees, out_degree_probability, filtered_out_degrees, out_intercept, out_slope, out_threshold = fit_power_law(out_degree_sequence)

# Create subplots
fig, axs = plt.subplots(1, 2, figsize=(12, 6))

# Plot in-degree distribution
axs[0].loglog(in_degrees, in_degree_probability, 'bo', markersize=5, alpha=0.6, label="In-degree")
axs[0].plot(np.exp(np.log(filtered_in_degrees)), np.exp(in_intercept + in_slope * np.log(filtered_in_degrees)), 'r--', linewidth=2, label='Linear Regression Line')

# Customize in-degree subplot
axs[0].set_title("In-Degree Distribution", size=16)
axs[0].set_xlabel("In-Degree (k)", size=12)
axs[0].set_ylabel("Probability (Pk)", size=12)
axs[0].legend()
axs[0].grid(True, which="both", ls="--", linewidth=0.5, alpha=0.6)

# Plot out-degree distribution
axs[1].loglog(out_degrees, out_degree_probability, 'ro', markersize=5, alpha=0.6, label="Out-degree")
axs[1].plot(np.exp(np.log(filtered_out_degrees)), np.exp(out_intercept + out_slope * np.log(filtered_out_degrees)), 'b--', linewidth=2, label='Linear Regression Line')

# Customize out-degree subplot
axs[1].set_title("Out-Degree Distribution", size=16)
axs[1].set_xlabel("Out-Degree (k)", size=12)
axs[1].set_ylabel("Probability (Pk)", size=12)
axs[1].legend()
axs[1].grid(True, which="both", ls="--", linewidth=0.5, alpha=0.6)

# Adjust layout and show plot
plt.tight_layout()
plt.show()

# Print the slopes of the linear regression lines (log-log) and thresholds
print("In-degree threshold (k):", in_threshold)
print("Slope of the in-degree linear regression line (log-log):", in_slope)
print("Out-degree threshold (k):", out_threshold)
print("Slope of the out-degree linear regression line (log-log):", out_slope)

# Assuming you have already computed in_degree_sequence, out_degree_sequence, and initialized G

# Compute in-degree distribution
unique_in_degrees, in_degree_counts = np.unique(in_degree_sequence, return_counts=True)
in_degree_probability = in_degree_counts / len(in_degree_sequence)
in_cumulative_distribution = np.cumsum(in_degree_probability[::-1])[::-1]  # Reverse cumulative distribution

# Compute out-degree distribution
unique_out_degrees, out_degree_counts = np.unique(out_degree_sequence, return_counts=True)
out_degree_probability = out_degree_counts / len(out_degree_sequence)
out_cumulative_distribution = np.cumsum(out_degree_probability[::-1])[::-1]  # Reverse cumulative distribution

# Create subplots
fig, axs = plt.subplots(1, 2, figsize=(12, 6))

# Plot in-degree distribution and cumulative distribution
axs[0].loglog(unique_in_degrees, in_degree_probability, 'bo', markersize=5, alpha=0.6, label="Degree Distribution")
axs[0].loglog(unique_in_degrees, in_cumulative_distribution, 'b--', linewidth=2, label="Cumulative Distribution")

# Customize in-degree subplot
axs[0].set_title("In-Degree Distribution", size=16)
axs[0].set_xlabel("In-Degree (k)", size=12)
axs[0].set_ylabel("Probability (Pk) / Cumulative Probability", size=12)
axs[0].legend()
axs[0].grid(True, which="both", ls="--", linewidth=0.5, alpha=0.6)

# Plot out-degree distribution and cumulative distribution
axs[1].loglog(unique_out_degrees, out_degree_probability, 'ro', markersize=5, alpha=0.6, label="Degree Distribution")
axs[1].loglog(unique_out_degrees, out_cumulative_distribution, 'r--', linewidth=2, label="Cumulative Distribution")

# Customize out-degree subplot
axs[1].set_title("Out-Degree Distribution", size=16)
axs[1].set_xlabel("Out-Degree (k)", size=12)
axs[1].set_ylabel("Probability (Pk) / Cumulative Probability", size=12)
axs[1].legend()
axs[1].grid(True, which="both", ls="--", linewidth=0.5, alpha=0.6)

# Adjust layout and show plot
plt.tight_layout()
plt.show()

# Function to compute cumulative distribution and fit power-law
def fit_cumulative_power_law(degree_sequence, threshold):
    unique_degrees, degree_counts = np.unique(degree_sequence, return_counts=True)
    degree_probability = degree_counts / len(degree_sequence)
    cumulative_distribution = np.cumsum(degree_probability[::-1])[::-1]  # Reverse cumulative distribution

    # Find the index where degrees are greater than or equal to threshold
    threshold_index = np.argmax(unique_degrees >= threshold)

    # Select data above the threshold
    filtered_degrees = unique_degrees[threshold_index:]
    filtered_cumulative_distribution = cumulative_distribution[threshold_index:]

    # Fit a linear regression line to the log-log plot of degree vs. cumulative probability
    log_degree = np.log(filtered_degrees).reshape(-1, 1)
    log_cumulative = np.log(filtered_cumulative_distribution)
    regression_model = LinearRegression().fit(log_degree, log_cumulative)

    # Get the slope and intercept of the linear regression line
    slope = regression_model.coef_[0]
    intercept = regression_model.intercept_

    return filtered_degrees, filtered_cumulative_distribution, slope, intercept

threshold = 24

# Compute and fit cumulative power-law for in-degree
filtered_in_degrees, in_cumulative_distribution, in_slope, in_intercept = fit_cumulative_power_law(in_degree_sequence, threshold)

# Compute and fit cumulative power-law for out-degree
filtered_out_degrees, out_cumulative_distribution, out_slope, out_intercept = fit_cumulative_power_law(out_degree_sequence, threshold)

# Create subplots
fig, axs = plt.subplots(1, 2, figsize=(12, 6))

# Plot in-degree cumulative distribution and linear fit
axs[0].loglog(filtered_in_degrees, in_cumulative_distribution, 'bo', markersize=5, alpha=0.6, label="In-Degree Cumulative Distribution")
axs[0].plot(np.exp(np.log(filtered_in_degrees)), np.exp(in_intercept + in_slope * np.log(filtered_in_degrees)), 'r--', linewidth=2, label='Linear Regression Line')

# Customize in-degree subplot
axs[0].set_title("In-Degree Cumulative Distribution", size=16)
axs[0].set_xlabel("In-Degree (k)", size=12)
axs[0].set_ylabel("Cumulative Probability", size=12)
axs[0].legend()
axs[0].grid(True, which="both", ls="--", linewidth=0.5, alpha=0.6)

# Plot out-degree cumulative distribution and linear fit
axs[1].loglog(filtered_out_degrees, out_cumulative_distribution, 'ro', markersize=5, alpha=0.6, label="Out-Degree Cumulative Distribution")
axs[1].plot(np.exp(np.log(filtered_out_degrees)), np.exp(out_intercept + out_slope * np.log(filtered_out_degrees)), 'b--', linewidth=2, label='Linear Regression Line')

# Customize out-degree subplot
axs[1].set_title("Out-Degree Cumulative Distribution", size=16)
axs[1].set_xlabel("Out-Degree (k)", size=12)
axs[1].set_ylabel("Cumulative Probability", size=12)
axs[1].legend()
axs[1].grid(True, which="both", ls="--", linewidth=0.5, alpha=0.6)

# Adjust layout and show plot
plt.tight_layout()
plt.show()

# Print the slopes of the linear regression lines (log-log)
print("Slope of the in-degree cumulative linear regression line (log-log):", in_slope)
print("Slope of the out-degree cumulative linear regression line (log-log):", out_slope)

# Sample data
diameter = 6
path_between_nodes = [['claudiatenney', 'RepStefanik'], ['RepStefanik', 'RepRichHudson'], ['RepRichHudson', 'RepAdams'], ['RepAdams', 'RepBeatty'], ['RepBeatty', 'RepJeffries'], ['RepJeffries', 'Kilili_Sablan']]
average_path_length = 2.357151853751136
is_connected = False  # Example: Change to True if your graph is connected

# Format data for table
table = [
    ["Diameter", diameter],
    ["Path between most distant nodes", path_between_nodes],
    ["Average path length", f"{average_path_length:.2f}"],
    ["Is the graph connected", "Yes" if is_connected else "No"]
]

# Print table using tabulate
print(tabulate(table, headers=["Property", "Value"], tablefmt="fancy_grid"))

g = ig.Graph.from_networkx(G1)
ig.plot(g)

#dict of X11 color names
colors = ig.drawing.colors.known_colors
colors = list(colors.keys())
G1_igraph = ig.Graph.Adjacency((nx.to_numpy_array(G1) > 0).tolist())

# Set vertex names
G1_igraph.vs["name"] = [str(node) for node in G1.nodes()]

# Set vertex labels to node names
G1_igraph.vs["label"] = G1_igraph.vs["name"]

# Plot the graph with clustering results using igraph
visual_style = {}
# Assign colors based on cluster labels
G1_igraph.vs["color"] = colors

visual_style["vertex_size"] = 20
#node label size
#visual_style["vertex_label_size"] = [degree for degree in g.degree()]
visual_style["vertex_label_size"] = 6
#edge thickness
#visual_style["edge_width"] = [0.14 * int(weight) for weight in g.es["weight"]]
#bounding box
visual_style["bbox"] = (500, 500)
#margin
visual_style["margin"] = 20
# Set arrow size
visual_style["edge_arrow_size"] = 0.6

ig.plot(G1_igraph, **visual_style,layout="kk")

# Assuming G1 is your NetworkX graph
G1_igraph = ig.Graph.Adjacency((nx.to_numpy_array(G1) > 0).tolist())

# Set vertex names
G1_igraph.vs["name"] = [str(node) for node in G1.nodes()]

# Set vertex labels to node names
G1_igraph.vs["label"] = G1_igraph.vs["name"]

# Example: Assign random colors to vertices based on their index
num_vertices = len(G1_igraph.vs)
random_colors = ['#{:02x}{:02x}{:02x}'.format(np.random.randint(0, 255), np.random.randint(0, 255), np.random.randint(0, 255)) for _ in range(num_vertices)]
G1_igraph.vs["color"] = random_colors

# Plot the graph with clustering results using igraph
visual_style = {
    "vertex_size": 10,             # Adjust vertex size for clarity
    "vertex_label_size": 8,        # Adjust vertex label size for readability
    "edge_arrow_size": 0.6,        # Set arrow size for directed edges
    "bbox": (800, 800),            # Set bounding box size (width, height)
    "margin": 50                   # Set margin size
}

# Plot the graph using Kamada-Kawai layout
ig.plot(G1_igraph, layout="kk", **visual_style)



"""**Size**"""

# GRAPH ORDER = NO OF NODES

nodes = G1_igraph.vs()
edges = G1_igraph.es()

print("Graph order:", len(nodes))

# GRAPH SIZE = NO OF EDGES

print("Graph size:", len(edges))

# DENSITY - HOW CONNECTED ARE THE NODES? NO OF EDGES/NO OF POSSIBLE EDGES

print("Number of possible edges (N*(N-1)):", len(nodes)*(len(nodes)-1))
print("Graph density:", g.density())

# CONNECTEDNESS - EASIEST TO INSPECT VISUALLY

print("Is the graph connected:","yes" if g.is_connected() else "no")

"""**Network diameter**"""

# DIAMETER - HOW FAR ARE THE TWO MOST DISTANT NODES

print("Network diameter:", G1_igraph.diameter(directed=True))
d = G1_igraph.get_diameter()
# GET NODES IN THE DIAMETER PATH
diameter_path = []
for i in range(0, G1_igraph.diameter()):
  diameter_path.append((d[i], d[i+1]))
# GET EDGES IN THE DIAMETER PATH
diameter_edges = G1_igraph.get_eids(pairs=diameter_path, directed=True)


#COLOR THE DIAMETER PATH
visual_style["vertex_color"] = ["red" if node.index in diameter_path else "white" for node in nodes]
visual_style["edge_color"] = ["red" if edge.index in diameter_edges else "black" for edge in edges]
# Draw the diameter path edges with a different color and thicker line


print("Path between most distant nodes:", [nodes[index]["label"] for index in diameter_path])

# AVERAGE PATH LENGTH - HOW CLOSE ARE THE NODES TO EACH OTHER ON AVERAGE

print("Average path length:", G1_igraph.average_path_length(directed=True))

ig.plot(G1_igraph, **visual_style)
#nx.draw_networkx_edges(G, edgelist=diameter_edges, edge_color='red', width=2)

#plot = ig.plot(g, **visual_style)

# Clustering coefficient
clustering_coefficient = nx.average_clustering(G)
print("Average clustering coefficient:", clustering_coefficient)

plt.style.use('seaborn-v0_8-colorblind')
G_igraph = ig.Graph.Adjacency((nx.to_numpy_array(G1) > 0).tolist())
gcopy = G_igraph.copy()


# Set vertex names
G1_igraph.vs["name"] = [str(node) for node in G1.nodes()]

# Set vertex labels to node names
G1_igraph.vs["label"] = G1_igraph.vs["name"]

# Assuming G1_igraph is your igraph.Graph object with the network structure

# Compute the diameter and get the diameter path
diameter_length = G1_igraph.diameter(directed=True)
print("Network diameter:", diameter_length)

# Get the vertices in the diameter path
diameter_path = G1_igraph.get_diameter()

# Get the edges in the diameter path
diameter_edges = []
for i in range(len(diameter_path) - 1):
    edge = G1_igraph.get_eid(diameter_path[i], diameter_path[i + 1], directed=True)
    diameter_edges.append(edge)


# Create a mapping from original vertex indices to subgraph vertex indices
vertex_mapping = {original_index: subgraph_index for subgraph_index, original_index in enumerate(diameter_path)}
# Find the edges between the most distant nodes (considering both directions)
edges_between_nodes = []
for node1 in diameter_path:
    for node2 in diameter_path:
        if node1 != node2:
            shortest_paths = G1_igraph.get_all_shortest_paths(node1, to=node2, mode=ig.OUT)
            for path in shortest_paths:
                for i in range(len(path) - 1):
                    edge = G1_igraph.get_eid(path[i], path[i + 1], directed=True)
                    edges_between_nodes.append(edge)

# Remove duplicate edges
edges_between_nodes = list(set(edges_between_nodes))

# Define visual style
visual_style = {
    "vertex_size": 20,   # Adjust vertex size
    "vertex_color": ["red" if node in diameter_path else "gray" for node in range(len(G1_igraph.vs))],  # Color nodes in diameter path red
    "edge_color": ["red" if edge in diameter_edges else "blue" if edge in edges_between_nodes else "gray" for edge in range(len(G1_igraph.es))],  # Color edges accordingly
    "edge_width": [2 if edge in diameter_edges or edge in edges_between_nodes else 1 for edge in range(len(G1_igraph.es))],  # Thicker edge for diameter path and edges between nodes
    "vertex_label_size": 6,  # Set vertex label size
 # Set bounding box size
    "margin": 50         # Set margin size
}

gcopy.delete_edges(edges)

layout = gcopy.layout("kk")



# Plot the graph
layout = G1_igraph.layout("kk")  # Use Kamada-Kawai layout for better visualization
#ig.plot(G1_igraph, **visual_style, layout=layout)

ig.plot(G1_igraph, **visual_style)

plt.style.use('seaborn-v0_8-colorblind')

# Convert NetworkX graph to igraph graph
G1_igraph = ig.Graph.Adjacency((nx.to_numpy_array(G1) > 0).tolist())


# Set vertex names
G1_igraph.vs["name"] = [str(node) for node in G1.nodes()]

# Set vertex labels to node names
G1_igraph.vs["label"] = G1_igraph.vs["name"]
# Assign names to the vertices
for i, vertex in enumerate(G1_igraph.vs):
    vertex["name"] = str(i)

# Compute the diameter and get the diameter path
diameter_length = G1_igraph.diameter(directed=True)
print("Network diameter:", diameter_length)

# Get the vertices in the diameter path
diameter_path = G1_igraph.get_diameter()

# Find the edges in the diameter path
diameter_edges = []
for i in range(len(diameter_path) - 1):
    edge = G1_igraph.get_eid(diameter_path[i], diameter_path[i + 1], directed=True)
    diameter_edges.append(edge)

# Extract the subgraph containing only the most distant nodes and the paths between them
subgraph_nodes = set(diameter_path)
subgraph = G1_igraph.subgraph(list(subgraph_nodes))

# Map the edges to the subgraph
subgraph_diameter_edges = [subgraph.es.find(_source=subgraph.vs.find(name=str(diameter_path[i])).index,
                                            _target=subgraph.vs.find(name=str(diameter_path[i + 1])).index).index
                           for i in range(len(diameter_path) - 1)]

# Set vertex names
G1_igraph.vs["name"] = [str(node) for node in G1.nodes()]

# Set vertex labels to node names
G1_igraph.vs["label"] = G1_igraph.vs["name"]
# Extract the subgraph containing only the most distant nodes and the paths between them
subgraph_nodes = set(diameter_path)
subgraph = G1_igraph.subgraph(list(subgraph_nodes))
subgraph.vs["label"] = subgraph.vs["name"]
# Define visual style for the subgraph
visual_style = {
    "vertex_size": 20,  # Adjust vertex size
    "vertex_color": "red",  # Color nodes in diameter path red
    "edge_color": ["blue" if edge.index in subgraph_diameter_edges else "gray" for edge in subgraph.es],  # Color diameter edges green
    "edge_width": [2 if edge.index in subgraph_diameter_edges else 1 for edge in subgraph.es],  # Thicker edge for diameter edges
    "vertex_label": [vertex["name"] for vertex in subgraph.vs],  # Label nodes with their names
    "vertex_label_size": 20,  # Set vertex label size
    "margin": 50  # Set margin size
}

# Plot the subgraph
layout = subgraph.layout("kk")  # Use Kamada-Kawai layout for better visualization
ig.plot(subgraph, "DD.png", **visual_style,layout=layout)

senators = ['claudiatenney', 'RepStefanik', 'RepRichHudson', 'RepAdams', 'RepBeatty',  'RepJeffries', 'Kilili_Sablan']
parties = []
for s in senators : parties.append((s,party_affiliations_dict[s]))
parties

isolated_nodes = list(nx.isolates(G))

if len(isolated_nodes)==0:
  print("There are no isolated nodes")
else:
  print(f"There are {len(isolated_nodes)} isolated nodes")

"""## **Interaction between the parties**"""

for i in range(0, len(G1.nodes())):
    G1.nodes[usernameList[i]]['party'] = party_affiliations[i]

# Step 3: Analyze cross-party interactions
intra_party_edges = {'Democratic': 0, 'Republican': 0, 'Independent': 0}
inter_party_edges = 0

for u, v in G1.edges():
    source_party = G1.nodes[u]['party']
    target_party = G1.nodes[v]['party']
    if source_party == target_party:
        intra_party_edges[source_party] += 1
    else:
        inter_party_edges += 1

total_edges = sum(intra_party_edges.values()) + inter_party_edges

print(f"Intra-party edges (Democratic): {intra_party_edges['Democratic']} ({intra_party_edges['Democratic'] / total_edges:.2%})")
print(f"Intra-party edges (Republican): {intra_party_edges['Republican']} ({intra_party_edges['Republican'] / total_edges:.2%})")
print(f"Intra-party edges (Independent): {intra_party_edges['Independent']} ({intra_party_edges['Independent'] / total_edges:.2%})")
print(f"Inter-party edges: {inter_party_edges} ({inter_party_edges / total_edges:.2%})")

# Step 5: Visualize the interaction network
# Define a color map for the parties
color_map = {'Democratic': 'blue', 'Republican': 'red', 'Independent': 'green'}
# Get node colors based on party affiliation
node_colors = [color_map[G1.nodes[node]['party']] for node in G1.nodes]

plt.figure(figsize=(20, 20))
# Use spring layout for the graph
pos = nx.spring_layout(G1, seed=42)  # for consistent layout
# Draw the network
nx.draw(G1, pos, node_color=node_colors, with_labels=True, node_size=500, font_size=10, edge_color='gray')
plt.title('Twitter Interaction Network of US Congress Members')
plt.show()

"""# **Community detection**"""

df = pd.DataFrame(data={'Algorithm':[], 'Number of communities':[], 'Modularity':[], 'Map equation':[]})

def plot_communities(graph, partition, file_name, removed_nodes):
  G_igraph = ig.Graph.Adjacency((nx.to_numpy_array(graph) > 0).tolist())
  gcopy = G_igraph.copy()
  edges = []
  edges_colors = []
  for edge in G_igraph.es():
      if partition[edge.tuple[0]] != partition[edge.tuple[1]]:
            edges.append(edge)
            edges_colors.append("gray")
      else:
            edges_colors.append("black")
  gcopy.delete_edges(edges)
  layout = gcopy.layout("kk")
  G_igraph.es["color"] = edges_colors
  visual_style = {}
  visual_style["vertex_label_dist"] = 0
  visual_style["vertex_shape"] = "circle"
  visual_style["edge_color"] = G_igraph.es["color"]
  visual_style["bbox"] = (4000, 2500)
  visual_style["vertex_size"] = 30
  visual_style["layout"] = layout
  visual_style["bbox"] = (3000, 2000)
  visual_style["margin"] = 40
  G_igraph.vs["name"] = [str(node) for node in graph.nodes()]

  # Set vertex labels to node names
  G_igraph.vs["label"] = G_igraph.vs["name"]

  colors = []
  for i in range(0, len(partition) + 1):
      colors.append('%06X' % randint(0, 0xFFFFFF))
  for vertex in G_igraph.vs():
      vertex["color"] = str('#') + colors[partition[vertex.index]]
  visual_style["vertex_color"] = G_igraph.vs["color"]

  ig.plot(G_igraph, file_name, **visual_style)

  image = plt.imread(file_name)

  # Plot the image
  plt.figure(figsize=(20, 25))  # Adjust figure size as needed
  labels = []
  legend_items = []
  if removed_nodes==True:
    for i in range(max(partition)-1):
      labels.append(f'Community {i+1}')
    labels.append('Removed nodes')
  else:
    for i in range(max(partition)):
      labels.append(f'Community {i+1}')
  for i in range(max(partition)):
      legend_items.append(plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='#' + colors[i+1], markersize=10, label=labels[i]))
  plt.legend(handles=legend_items, loc='upper left', fontsize=10)
  plt.imshow(image)
  plt.show()  # Display the plot

"""**Louvain algorithm**"""

louvain_output = nx.community.greedy_modularity_communities(G)

community_assignment = {}

# Assign community labels
for community_id, community in enumerate(louvain_output):
    for node in community:
        community_assignment[node] = community_id + 1

louvain_communities = [community_assignment[node] for node in sorted(community_assignment.keys())]

print("Original number of communities:")
print(np.max(louvain_communities))

G2 = G.copy()
community_counts = Counter(louvain_communities)

# Identify communities with fewer than 10 nodes
small_communities = {community for community, count in community_counts.items() if count < 10}

# Filter out nodes belonging to small communities
louvain_filtered = [label for label in louvain_communities if label not in small_communities]

nodes_to_remove = [node for node, community in zip(G2.nodes(), louvain_communities) if community in small_communities]

# Remove nodes and their edges from the graph
G2.remove_nodes_from(nodes_to_remove)

print(f"{len(nodes_to_remove)} nodes were removed")

isolated_names = [usernameList[i] for i in range(0, len(G.nodes())) if louvain_communities[i]>3]

isolated_names

senators = ['RepCindyAxne', 'SanfordBishop', 'RepChrisPappas']
parties = []
for s in senators : parties.append((s,party_affiliations_dict[s]))
parties

mixed_community_names = [usernameList[i] for i in range(0, len(G.nodes())) if louvain_communities[i]==3]

mixed_community_names

print("Number of main communities:")
print(np.max(louvain_filtered))

louvain_modularity = nx.community.modularity(G2, nx.community.greedy_modularity_communities(G2))
print("Louvain Modularity:")
print(louvain_modularity)

L_louvain = map_equation(G2, louvain_filtered)
print("Map equation value:")
print(L_louvain)

Counter(louvain_filtered)

plot_communities(G1, louvain_communities, "louvain_plot.png", True)

data = pd.DataFrame({
    'community': louvain_communities,
    'party': party_affiliations
})

# Count the total number of nodes in each community
community_counts = data['community'].value_counts().sort_index()

# Add community counts to the dataset
data['community_count'] = data['community'].map(community_counts)

# Count the number of members for each party in each community
party_counts = data.groupby(['community', 'party']).size().unstack(fill_value=0)

# Convert counts to percentages
party_percentages = party_counts.div(party_counts.sum(axis=1), axis=0) * 100

# Plotting
plt.style.use('seaborn-v0_8-colorblind')
ax = party_percentages.iloc[:-1].plot(kind='bar', stacked=True, figsize=(10, 7))

plt.xlabel('Community')
plt.ylabel('Percentage')
plt.title('Party Percentage for Each Party in Each Community (Louvain)')
plt.legend(title='Party')
plt.show()

party_percentages['community_size'] = party_percentages.index.map(community_counts)
party_percentages.iloc[:-1]

df.loc[len(df)] = ["Louvain", np.max(louvain_filtered), louvain_modularity, L_louvain]

subgraph = G.subgraph([i for i in range(0, len(G.nodes())) if louvain_communities[i]==3]).copy()

# Apply the Louvain community detection algorithm
louvain_subcommunities_output = nx.community.greedy_modularity_communities(subgraph)

community_assignment = {}

# Assign community labels
for community_id, community in enumerate(louvain_subcommunities_output):
    for node in community:
        community_assignment[node] = community_id + 1

louvain_subcommunities = [community_assignment[node] for node in sorted(community_assignment.keys())]

Counter(louvain_subcommunities)

party_affiliations2=[]
for i in range(0, len(G.nodes())):
  if louvain_communities[i]==3:
    party_affiliations2.append(party_affiliations[i])

data_subcommunities = pd.DataFrame({
    'community': louvain_subcommunities,
    'party': party_affiliations2
})

# Count the total number of nodes in each community
community_counts = data_subcommunities['community'].value_counts().sort_index()

# Add community counts to the dataset
data_subcommunities['community_count'] = data_subcommunities['community'].map(community_counts)

# Count the number of members for each party in each community
party_counts = data_subcommunities.groupby(['community', 'party']).size().unstack(fill_value=0)

# Convert counts to percentages
party_percentages = party_counts.div(party_counts.sum(axis=1), axis=0) * 100

plt.style.use('seaborn-v0_8-colorblind')
ax = party_percentages.iloc[:-2].plot(kind='bar', stacked=True, figsize=(10, 7))

plt.xlabel('Community')
plt.ylabel('Percentage')
plt.title('Party Percentage for Each Party in the subcommunities (Louvain)')
plt.legend(title='Party')
plt.show()

party_percentages['community_size'] = party_percentages.index.map(community_counts)
party_percentages.iloc[:-2]

"""**Infomap algorithm**"""

im = Infomap("--two-level", directed=True)

im.read_file("congress.edgelist")

im.run()

infomap_communities = [im.get_modules(node) for node in range(len(G))][-1]
infomap_communities = np.array(list(infomap_communities.values()))

print("Original number of communities:")
print(np.max(infomap_communities))

G2 = G.copy()
community_counts = Counter(infomap_communities)

# Identify communities with fewer than 10 nodes
small_communities = {community for community, count in community_counts.items() if count < 10}

# Filter out nodes belonging to small communities
infomap_filtered = [label for label in infomap_communities if label not in small_communities]

nodes_to_remove = [node for node, community in zip(G2.nodes(), infomap_communities) if community in small_communities]

# Remove nodes and their edges from the graph
G2.remove_nodes_from(nodes_to_remove)

isolated_names = [usernameList[i] for i in range(0, len(G.nodes())) if infomap_communities[i]>3]

isolated_names

senators = ['RepLizCheney',
 'RepJohnCurtis',
 'RepKayGranger',
 'RepElaineLuria',
 'RepOHalleran',
 'RepChrisPappas',
 'RepTomSuozzi',
 'RepAnnWagner',
 'RepJoeWilson',
 'RobWittman']
parties = []
for s in senators : parties.append((s,party_affiliations_dict[s]))
parties

mixed_community_names = [usernameList[i] for i in range(0, len(G.nodes())) if infomap_communities[i]==3]

mixed_community_names

print(f"{len(nodes_to_remove)} nodes were removed")

community_dict = defaultdict(list)

# Populate the dictionary
for node, label in enumerate(infomap_filtered):
    community_dict[label].append(node)

# Convert the dictionary values to frozen sets
infomap_output = [frozenset(nodes) for nodes in community_dict.values()]

print("Number of communities:")
print(np.max(infomap_filtered))

infomap_communities2 = [infomap_communities[i] if infomap_communities[i] <= np.max(infomap_filtered) else np.max(infomap_filtered) + 1 for i in range(len(G))]

infomap_modularity = nx.community.modularity(nx.from_numpy_array(nx.to_numpy_array(G2)), infomap_output)
print("Infomap modularity:")
print(infomap_modularity)

L_infomap = im.codelength
print("Map equation value:")
print(L_infomap)

Counter(infomap_filtered)

plot_communities(G1, infomap_communities2, "infomap_plot.png", True)

data = pd.DataFrame({
    'community': infomap_communities,
    'party': party_affiliations
})

# Count the total number of nodes in each community
community_counts = data['community'].value_counts().sort_index()

# Add community counts to the dataset
data['community_count'] = data['community'].map(community_counts)

# Count the number of members for each party in each community
party_counts = data.groupby(['community', 'party']).size().unstack(fill_value=0)

# Convert counts to percentages
party_percentages = party_counts.div(party_counts.sum(axis=1), axis=0) * 100

plt.style.use('seaborn-v0_8-colorblind')
ax = party_percentages.iloc[:-5].plot(kind='bar', stacked=True, figsize=(10, 7))

plt.xlabel('Community')
plt.ylabel('Percentage')
plt.title('Party Percentage for Each Party in Each Community (Infomap)')
plt.legend(title='Party')
plt.show()

party_percentages['community_size'] = party_percentages.index.map(community_counts)
party_percentages.iloc[:-5]

df.loc[len(df)] = ["Infomap", np.max(infomap_filtered), infomap_modularity, L_infomap]

subgraph = G.subgraph([i for i in range(0, len(G.nodes())) if infomap_communities[i]==3]).copy()

im = Infomap("--two-level", directed=True)
for edge in subgraph.edges():
    im.add_link(*edge)

im.run()

infomap_subcommunities = [im.get_modules(node) for node in range(len(G))][-1]
infomap_subcommunities = np.array(list(infomap_subcommunities.values()))

print("Number of subcommunities:")
print(np.max(infomap_subcommunities))

"""**Girvan-Newman**"""

communities = list(nx.community.girvan_newman(G))

# Modularity -> measures the strength of division of a network into modules
modularity_df = pd.DataFrame(
    [
        [k + 1, nx.community.modularity(G, communities[k])]
        for k in range(len(communities))
    ],
    columns=["k", "modularity"],
)

highest_modularity_row = modularity_df.loc[modularity_df['modularity'].idxmax()]

print("Girvan-Newman modularity:")
print(highest_modularity_row['modularity'])

modularity_df.plot(kind='scatter', x='k', y='modularity', s=32, alpha=.8)
plt.gca().spines[['top', 'right',]].set_visible(False)

clustering_dict = {}

# Assign cluster labels to elements
for i, s in enumerate(communities[int(highest_modularity_row['k'])]):
    for element in s:
        clustering_dict[element] = i+1

GN_communities = [clustering_dict[element] for element in range(len(G.nodes()))]

print("Original number of communities:")
print(np.max(GN_communities))

G2 = G.copy()
community_counts = Counter(GN_communities)

# Identify communities with fewer than 10 nodes
small_communities = {community for community, count in community_counts.items() if count < 10}

# Filter out nodes belonging to small communities
GN_filtered = [label for label in louvain_communities if label not in small_communities]

nodes_to_remove = [node for node, community in zip(G2.nodes(), louvain_communities) if community in small_communities]

# Remove nodes and their edges from the graph
G2.remove_nodes_from(nodes_to_remove)

isolated_names = [usernameList[i] for i in range(0, len(G.nodes())) if GN_communities[i]>2]

isolated_names

senators = ['SenDanSullivan',
 'RepEdCase',
 'RepRonEstes',
 'RepKayGranger',
 'RepJoshHarder',
 'RepKirkpatrick',
 'RepAlLawsonJr',
 'RepOHalleran',
 'Kilili_Sablan',
 'RepSchrader',
 'CongMikeSimpson',
 'RepTomSuozzi']
parties = []
for s in senators : parties.append((s,party_affiliations_dict[s]))
parties

print("Number of communities:")
print(np.max(GN_filtered))

print("Girvan-Newman modularity:")
print(highest_modularity_row['modularity'])

L_GN = map_equation(G2, GN_filtered)
print("Map equation value:")
print(L_GN)

GN_communities2 = [GN_communities[i] if GN_communities[i] <= np.max(GN_filtered) else np.max(GN_filtered) + 1 for i in range(len(G))]

Counter(GN_filtered)

plot_communities(G1, GN_communities2, "GN_plot.png", removed_nodes=True)

data = pd.DataFrame({
    'community': GN_communities,
    'party': party_affiliations
})

# Count the total number of nodes in each community
community_counts = data['community'].value_counts().sort_index()

# Add community counts to the dataset
data['community_count'] = data['community'].map(community_counts)

# Count the number of members for each party in each community
party_counts = data.groupby(['community', 'party']).size().unstack(fill_value=0)

# Convert counts to percentages
party_percentages = party_counts.div(party_counts.sum(axis=1), axis=0) * 100

# Plotting
ax = party_percentages[:2].plot(kind='bar', stacked=True, figsize=(10, 7))

plt.xlabel('Community')
plt.ylabel('Percentage')
plt.title('Party Percentage for Each Party in Each Community (Girvan-Newman)')
plt.legend(title='Party')
plt.show()

party_percentages = party_percentages.iloc[:2]

party_percentages['community_size'] = party_percentages.index.map(community_counts)
party_percentages

df.loc[len(df)] = ["Girvan-Newman", np.max(GN_filtered), highest_modularity_row['modularity'], L_GN]

"""**Spectral clustering**"""

# Function to compute Gaussian similarity
def gaussian_similarity(X, sigma=1.0):
    pairwise_sq_dists = np.square(pairwise_distances(X, metric='euclidean'))
    return np.exp(-pairwise_sq_dists / (2.0 * sigma ** 2))

# Function to compute cosine similarity with handling for divisions by zero
def cosine_similarity(X):
    pairwise_cos_dists = pairwise_distances(X, metric='cosine')
    epsilon = 1e-9  # Small epsilon value to avoid divisions by zero
    return 1 - pairwise_cos_dists / (pairwise_cos_dists.max() + epsilon)

# Function to compute nearest neighbors similarity from a graph
def nearest_neighbors_similarity_from_graph(G, k):
    A = nx.to_numpy_array(G)
    degrees = np.sum(A, axis=1)
    A /= degrees[:, None]  # Normalize adjacency matrix
    A_knn = np.zeros_like(A)
    for i in range(len(A)):
        indices = np.argsort(A[i])[-k:]  # Indices of k nearest neighbors
        A_knn[i, indices] = A[i, indices]
    return A_knn

# Get node positions using spring layout
position = np.array(list(nx.spring_layout(G1).values()))

# Convert node positions to 2D array
X = A.toarray()
# Compute the similarity matrix using Gaussian similarity
sigma = 0.1  # Sigma parameter for Gaussian similarity
similarity_matrix = gaussian_similarity(X, sigma)

# Perform spectral clustering
n_clusters = 3  # Number of clusters
cluster_labels = SpectralClustering(n_clusters=n_clusters, affinity='precomputed', assign_labels='kmeans').fit_predict(similarity_matrix)


# Compute the similarity matrix using cosine similarity
similarity_matrix_cosine = cosine_similarity(X)

# Perform spectral clustering
cluster_labels_cosine = SpectralClustering(n_clusters=n_clusters, affinity='precomputed', assign_labels='kmeans').fit_predict(similarity_matrix_cosine)


# Compute the similarity matrix using nearest neighbors similarity
k = 10  # Number of nearest neighbors
similarity_matrix_nearest_neighbors = nearest_neighbors_similarity_from_graph(G, k)

# Perform spectral clustering
n_clusters = 3  # Number of clusters
cluster_labels_nearest_neighbors = SpectralClustering(n_clusters=n_clusters, affinity='precomputed', assign_labels='kmeans').fit_predict(similarity_matrix_nearest_neighbors)


print("spectral clustering with nearest neighbors similarity")
# Example usage with silhouette score
silhouette = silhouette_score(X, cluster_labels)
print("Silhouette Score:", silhouette)
# Example usage with Davies-Bouldin Index
db_index = davies_bouldin_score(X, cluster_labels)
print("Davies-Bouldin Index:", db_index)
# Example usage with Calinski-Harabasz Index
ch_index = calinski_harabasz_score(X, cluster_labels)
print("Calinski-Harabasz Index:", ch_index,"\n")


print("spectral clustering with Gaussian similarity")
# Example usage with silhouette score
silhouette = silhouette_score(X, cluster_labels_nearest_neighbors)
print("Silhouette Score:", silhouette)
# Example usage with Davies-Bouldin Index
db_index = davies_bouldin_score(X, cluster_labels_nearest_neighbors)
print("Davies-Bouldin Index:", db_index)
# Example usage with Calinski-Harabasz Index
ch_index = calinski_harabasz_score(X, cluster_labels_nearest_neighbors)
print("Calinski-Harabasz Index:", ch_index, "\n")


print("spectral clustering with cosine similarity")
# Example usage with silhouette score
silhouette = silhouette_score(X, cluster_labels_cosine)
print("Silhouette Score:", silhouette)
# Example usage with Davies-Bouldin Index
db_index = davies_bouldin_score(X, cluster_labels_cosine )
print("Davies-Bouldin Index:", db_index)
# Example usage with Calinski-Harabasz Index
ch_index = calinski_harabasz_score(X, cluster_labels_cosine )
print("Calinski-Harabasz Index:", ch_index,"\n")

community_dict = defaultdict(list)

# Populate the dictionary
for node, label in enumerate(cluster_labels):
    community_dict[label].append(node)

# Convert the dictionary values to frozen sets
sc1_output = [frozenset(nodes) for nodes in community_dict.values()]
df.loc[len(df)] = ["Spectral clustering with Gaussian similarity", np.max(cluster_labels) + 1, nx.community.modularity(G, sc1_output), map_equation(G, cluster_labels)]

community_dict = defaultdict(list)

# Populate the dictionary
for node, label in enumerate(cluster_labels_nearest_neighbors):
    community_dict[label].append(node)

# Convert the dictionary values to frozen sets
sc2_output = [frozenset(nodes) for nodes in community_dict.values()]
df.loc[len(df)] = ["Spectral clustering with nearest neighbors similarity", np.max(cluster_labels_nearest_neighbors) + 1, nx.community.modularity(G, sc2_output), map_equation(G, cluster_labels_nearest_neighbors)]

community_dict = defaultdict(list)

# Populate the dictionary
for node, label in enumerate(cluster_labels_cosine):
    community_dict[label].append(node)

# Convert the dictionary values to frozen sets
sc3_output = [frozenset(nodes) for nodes in community_dict.values()]
df.loc[len(df)] = ["Spectral clustering with cosine similarity", np.max(cluster_labels_cosine) + 1, nx.community.modularity(G, sc3_output), map_equation(G, cluster_labels_cosine)]

"""**Gaussian similarity**"""

data = pd.DataFrame({
    'community': cluster_labels,
    'party': party_affiliations
})

# Count the total number of nodes in each community
community_counts = data['community'].value_counts().sort_index()

# Add community counts to the dataset
data['community_count'] = data['community'].map(community_counts)

# Count the number of members for each party in each community
party_counts = data.groupby(['community', 'party']).size().unstack(fill_value=0)

# Convert counts to percentages
party_percentages = party_counts.div(party_counts.sum(axis=1), axis=0) * 100

# Plotting
plt.style.use('seaborn-v0_8-colorblind')
ax = party_percentages.plot(kind='bar', stacked=True, figsize=(10, 7))

plt.xlabel('Community')
plt.ylabel('Percentage')
plt.title('Party Percentage for Each Party in Each Community (Spectral clustering with Gaussian similarity)')
plt.legend(title='Party')
plt.show()

party_percentages['community_size'] = party_percentages.index.map(community_counts)
party_percentages

cluster_labels = [cluster_labels[i] + 1 for i in range(0, len(cluster_labels))]

plot_communities(G1, cluster_labels, "SC_Gaussian_plot.png", removed_nodes = False)

"""**Nearest neighbors similarity**"""

data = pd.DataFrame({
    'community': cluster_labels_nearest_neighbors,
    'party': party_affiliations
})

# Count the total number of nodes in each community
community_counts = data['community'].value_counts().sort_index()

# Add community counts to the dataset
data['community_count'] = data['community'].map(community_counts)

# Count the number of members for each party in each community
party_counts = data.groupby(['community', 'party']).size().unstack(fill_value=0)

# Convert counts to percentages
party_percentages = party_counts.div(party_counts.sum(axis=1), axis=0) * 100

# Plotting
ax = party_percentages.plot(kind='bar', stacked=True, figsize=(10, 7))

plt.xlabel('Community')
plt.ylabel('Percentage')
plt.title('Party Percentage for Each Party in Each Community (Spectral clustering with 10-Nearest Neighbors)')
plt.legend(title='Party')
plt.show()

party_percentages['community_size'] = party_percentages.index.map(community_counts)
party_percentages

cluster_labels_nearest_neighbors = [cluster_labels_nearest_neighbors[i] + 1 for i in range(0, len(cluster_labels_nearest_neighbors))]

plot_communities(G1, cluster_labels_nearest_neighbors, "SC_nearest_neighbors_plot.png", False)

"""**Cosine similarity**"""

data = pd.DataFrame({
    'community': cluster_labels_cosine,
    'party': party_affiliations
})

# Count the total number of nodes in each community
community_counts = data['community'].value_counts().sort_index()

# Add community counts to the dataset
data['community_count'] = data['community'].map(community_counts)

# Count the number of members for each party in each community
party_counts = data.groupby(['community', 'party']).size().unstack(fill_value=0)

# Convert counts to percentages
party_percentages = party_counts.div(party_counts.sum(axis=1), axis=0) * 100

# Plotting
ax = party_percentages.plot(kind='bar', stacked=True, figsize=(10, 7))

plt.xlabel('Community')
plt.ylabel('Percentage')
plt.title('Party Percentage for Each Party in Each Community (Spectral clustering with cosine similarity)')
plt.legend(title='Party')
plt.show()

party_percentages['community_size'] = party_percentages.index.map(community_counts)
party_percentages

cluster_labels_cosine = [cluster_labels_cosine[i] + 1 for i in range(0, len(cluster_labels_cosine))]

plot_communities(G1, cluster_labels_cosine, "SC_cosine_plot.png", False)

df

"""# **Small world**

**Small World metrics**
"""

def omega_(G):
    # Compute clustering coefficient (C) and average shortest path length (L) of the network
    C = nx.average_clustering(G)
    L = nx.average_shortest_path_length(G)

    # Generate latticized and randomized networks
    num_nodes = len(G)
    p_lattice = nx.watts_strogatz_graph(num_nodes, k=4, p=0.1)  # Adjust parameters as needed
    p_randomized = nx.double_edge_swap(G, nswap=num_nodes*10, max_tries=num_nodes*100)  # Adjust parameters as needed

    # Compute clustering coefficient (Cl) and average shortest path length (Lr) of the latticized network
    Cl = nx.average_clustering(p_lattice)
    Lr = nx.average_shortest_path_length(p_randomized)

    # Compute omega
    omega_value = (C / Cl) / (L / Lr)

    return omega_value


def small_world_metrics(G):
    # Convert the graph to undirected
    G_undirected = G.to_undirected()

    # Compute clustering coefficient and average shortest path length
    clustering_coefficient = nx.average_clustering(G_undirected)
    avg_shortest_path_length = nx.average_shortest_path_length(G_undirected)

    # Generate randomized and latticized networks
    num_nodes = len(G_undirected)
    #p_randomized = random_reference(G_undirected, niter=1, connectivity=True, seed=None)
    #p_lattice = lattice_reference(G_undirected, niter=5, D=None, connectivity=True, seed=None)

    # Compute clustering coefficient and average shortest path length for randomized and latticized networks
    #clustering_coefficient_randomized = nx.average_clustering(p_randomized)
    #avg_shortest_path_length_lattice = nx.average_shortest_path_length(p_lattice)

    # Normalize clustering coefficient and average shortest path length
    gamma = clustering_coefficient
    lambda_ = avg_shortest_path_length

    # Compute small world index
    sigma = clustering_coefficient / avg_shortest_path_length

    # Compute alternative metric
    omega = omega_(G_undirected)

    return gamma, lambda_, sigma, omega

gamma, lambda_, sigma, omega = small_world_metrics(G1)


print("Normalized Clustering Coefficient ():", gamma)
print("Normalized Path Length ():", lambda_)
print("Small World Index ():", sigma)
print("Alternative Metric ():", omega)

"""**Random graph**"""

print("Average Clustering Coefficient:", clustering_coefficient)

if nx.is_strongly_connected(G):
    avg_shortest_path_length = nx.average_shortest_path_length(G)
else:
    # Get the largest strongly connected component
    largest_scc = max(nx.strongly_connected_components(G), key=len)
    G_sub = G.subgraph(largest_scc)
    avg_shortest_path_length = nx.average_shortest_path_length(G_sub)

print("Average Shortest Path Length:", avg_shortest_path_length)

edges = len(G.edges())
nodes = len(G.nodes())
probability = edges / (nodes * (nodes - 1))  # Probability for edge creation

random_graph = nx.gnp_random_graph(len(G.nodes()), probability, directed=True)

# Calculate clustering coefficient and shortest path length for the random directed graph
random_avg_clustering = nx.average_clustering(random_graph.to_undirected())

if nx.is_strongly_connected(random_graph):
    random_avg_shortest_path_length = nx.average_shortest_path_length(random_graph)
else:
    largest_scc_random = max(nx.strongly_connected_components(random_graph), key=len)
    random_sub = random_graph.subgraph(largest_scc_random)
    random_avg_shortest_path_length = nx.average_shortest_path_length(random_sub)

print("Random Directed Graph Average Clustering Coefficient:", random_avg_clustering)
print("Random Directed Graph Average Shortest Path Length:", random_avg_shortest_path_length)
mapping = {}
for i in range(0, len(usernameList)):
  mapping[i] = usernameList[i]
random_graph = nx.relabel_nodes(random_graph, mapping)

random_graph_ig = ig.Graph.Adjacency((nx.to_numpy_array(random_graph) > 0).tolist())
layout_random_graph = random_graph_ig.layout("kk")

random_graph_ig = ig.Graph.Adjacency((nx.to_numpy_array(random_graph) > 0).tolist())

# Set vertex names
random_graph_ig.vs["name"] = [str(node) for node in random_graph.nodes()]

# Set vertex labels to node names
random_graph_ig.vs["label"] = random_graph_ig.vs["name"]

# Example: Assign random colors to vertices based on their index
num_vertices = len(random_graph_ig.vs)
random_graph_ig.vs["color"] = "yellow"

# Plot the graph with clustering results using igraph
visual_style = {
    "vertex_size": 20,             # Adjust vertex size for clarity
    "vertex_label_size": 8,        # Adjust vertex label size for readability
    "edge_arrow_size": 0.6,        # Set arrow size for directed edges
    "bbox": (800, 800),            # Set bounding box size (width, height)
    "margin": 50                   # Set margin size
}

# Plot the graph using Kamada-Kawai layout
ig.plot(random_graph_ig, layout="kk", **visual_style)

"""**Watts-Strogatz Graph**"""

print("Average Clustering Coefficient:", clustering_coefficient)
print("Average Shortest Path Length:", avg_shortest_path_length)

n = G.number_of_nodes()
k = 2 * (edges // n)  # approximate each node has similar degree as original graph
p = 0.1  # rewiring probability

WS_graph = nx.watts_strogatz_graph(n, k, p)

WS_avg_clustering = nx.average_clustering(WS_graph)
if nx.is_connected(WS_graph):
    WS_avg_shortest_path_length = nx.average_shortest_path_length(WS_graph)
else:
    largest_cc = max(nx.connected_components(WS_graph), key=len)
    WS_sub = WS_graph.subgraph(largest_cc)
    WS_avg_shortest_path_length = nx.average_shortest_path_length(WS_sub)

print("Watts-Strogatz Graph - Average Clustering Coefficient:", WS_avg_clustering)
print("Watts-Strogatz Graph - Average Shortest Path Length:", WS_avg_shortest_path_length)

mapping = {}
for i in range(0, len(usernameList)):
  mapping[i] = usernameList[i]
WS_graph = nx.relabel_nodes(WS_graph, mapping)

WS_graph_ig = ig.Graph.Adjacency((nx.to_numpy_array(WS_graph) > 0).tolist())
layout_WS = WS_graph_ig.layout("kk")

WS_graph_ig = ig.Graph.Adjacency((nx.to_numpy_array(WS_graph) > 0).tolist())

# Set vertex names
WS_graph_ig.vs["name"] = [str(node) for node in WS_graph.nodes()]

# Set vertex labels to node names
WS_graph_ig.vs["label"] = WS_graph_ig.vs["name"]

# Example: Assign random colors to vertices based on their index
num_vertices = len(WS_graph_ig.vs)
WS_graph_ig.vs["color"] = "green"

# Plot the graph with clustering results using igraph
visual_style = {
    "vertex_size": 20,             # Adjust vertex size for clarity
    "vertex_label_size": 8,        # Adjust vertex label size for readability
    "edge_arrow_size": 0.6,        # Set arrow size for directed edges
    "bbox": (800, 800),            # Set bounding box size (width, height)
    "margin": 50                   # Set margin size
}

# Plot the graph using Kamada-Kawai layout
ig.plot(WS_graph_ig, layout="kk", **visual_style)

"""# **Centrality metrics**"""

# Degree centrality
degree_centrality = nx.degree_centrality(G)
print("Degree Centrality:", degree_centrality)

# PageRank
pagerank = nx.pagerank(G)
print("PageRank:", pagerank)

# HITS
hits = nx.hits(G)
print("HITS:", hits)

# Closeness
closeness = nx.closeness_centrality(G)
print("Closeness Centrality:", closeness)

# Betweenness
betweenness = nx.betweenness_centrality(G)
print("Betweenness Centrality:", betweenness)

# Clustering coefficient
clustering_coefficient = nx.clustering(G)
print("Clustering Coefficient:", clustering_coefficient)

# Function to create and visualize a graph
def create_graph(G):
    # Compute eigenvector centrality
    eigen = nx.eigenvector_centrality(G)

    # Create a new graph
    H = nx.Graph(G)

    # Draw the graph with positions
    pos = nx.random_layout(H)  # Using spring layout for better spacing

    # Adjusting node size based on centrality scores
    node_size = [10000 * round(float(w), 3) for w in eigen.values()]

    # Drawing nodes with colors and sizes
    nx.draw_networkx_nodes(H, pos, node_size=node_size, node_color=list(eigen.values()), cmap=plt.cm.Blues)

    # Drawing edges with reduced width
    nx.draw_networkx_edges(H, pos, width=0.5)

    # Adding labels if needed
    nx.draw_networkx_labels(H, pos, font_size=8, font_color='black', font_family='sans-serif', alpha=0.7)

    # Adding a colorbar
    sm = plt.cm.ScalarMappable(cmap=plt.cm.Blues, norm=plt.Normalize(vmin=min(eigen.values()), vmax=max(eigen.values())))
    sm._A = []
    plt.colorbar(sm, label='Eigenvector Centrality')

    # Removing axes for better appearance
    plt.axis('off')

    plt.show()

# Increase the size of the plot
plt.figure(figsize=(12, 8))

# Visualize the graph
create_graph(G1)

# Function to create and visualize a graph with different centrality measures
def create_graph(G, centrality_measure):
    # Create a new graph
    H = G.copy()
    # Compute the specified centrality measure
    if centrality_measure == "closeness":
        centrality = nx.closeness_centrality(G)
        # Draw the graph with positions
        pos = nx.random_layout(H)
        title = "Closeness Centrality"
    elif centrality_measure == "betweenness":
        centrality = nx.betweenness_centrality(G)
        # Draw the graph with positions
        pos = nx.random_layout(H)
        title = "Betweenness Centrality"
    elif centrality_measure == "degree":
        centrality = nx.degree_centrality(G)
        pos = nx.random_layout(H)
        title = "Degree Centrality"
    elif centrality_measure == "eigenvector":
        centrality = nx.eigenvector_centrality(G)
        pos = nx.random_layout(H)
        title = "Eigenvector Centrality"
    elif centrality_measure == "pagerank":
        centrality = nx.pagerank(G)
        pos = nx.random_layout(H)
        title = "PageRank"
    elif centrality_measure == "hits":
        hits = nx.hits(G)
        centrality = hits[0]
        pos = nx.random_layout(H)
        title = "HITS"
    else:
        raise ValueError("Invalid centrality measure. Choose from 'closeness', 'betweenness', or 'degree'.")





    # Adjusting node size based on centrality scores
    node_size = [3000 * v for v in centrality.values()]

    # Drawing nodes with colors and sizes
    nx.draw_networkx_nodes(H, pos, node_size=node_size, node_color=list(centrality.values()), cmap=plt.cm.Blues)

    # Drawing edges with reduced width
    nx.draw_networkx_edges(H, pos, width=0.1)

    # Adding labels if needed
    nx.draw_networkx_labels(H, pos, font_size=8, font_color='black', font_family='sans-serif', alpha=0.7)

    # Adding a colorbar
    sm = plt.cm.ScalarMappable(cmap=plt.cm.Blues, norm=plt.Normalize(vmin=min(centrality.values()), vmax=max(centrality.values())))
    sm._A = []

    plt.colorbar(sm, label=title)

    # Removing axes for better appearance
    plt.axis('off')

    # Set title
    plt.title(title)
    plt.show()

# Increase the size of the plot
plt.figure(figsize=(12, 8))
# Visualize the graph with closeness centrality
create_graph(G1, centrality_measure="closeness")

# Increase the size of the plot
plt.figure(figsize=(12, 8))
# Visualize the graph with degree centrality
create_graph(G1, centrality_measure="eigenvector")

# Increase the size of the plot
plt.figure(figsize=(12, 8))
# Visualize the graph with degree centrality
create_graph(G1, centrality_measure="degree")

plt.figure(figsize=(12, 8))
# Visualize the graph with degree centrality
create_graph(G1, centrality_measure="betweenness")

plt.figure(figsize=(12, 8))
# Visualize the graph with degree centrality
create_graph(G1, centrality_measure="pagerank")

plt.figure(figsize=(12, 8))
# Visualize the graph with degree centrality
create_graph(G1, centrality_measure="hits")

"""# **Centrality measurements over top 20 nodes degrees**"""

import networkx as nx
import igraph as ig
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler
from random import randint
import matplotlib.colors as mcolors

# Function to create and visualize a graph
def create_graph(G, centrality_measure=None, top_n=20):
    # Compute the specified centrality measure
    if centrality_measure == "eigenvector":
        centrality = nx.eigenvector_centrality(G)
    elif centrality_measure == "closeness":
        centrality = nx.closeness_centrality(G)
    elif centrality_measure == "betweenness":
        centrality = nx.betweenness_centrality(G)
    elif centrality_measure == "degree":
        centrality = nx.degree_centrality(G)
    elif centrality_measure == "hits":
        hits = nx.hits(G)
        centrality = hits[1]
    elif centrality_measure == "pagerank":
        centrality = nx.pagerank(G)
    else:
        raise ValueError("Invalid centrality measure. Choose from 'eigenvector', 'closeness', 'betweenness', or 'degree'.")


    G_igraph = ig.Graph.from_networkx(G)

    # Set vertex labels to node names
    G_igraph.vs["label"] = G_igraph.vs["_nx_name"]

    top_nodes = sorted(centrality, key=centrality.get, reverse=True)[:top_n]






    # Create a subgraph containing only the top nodes
    H = G.subgraph(top_nodes)

    # Convert NetworkX graph to igraph
    H_igraph = ig.Graph.from_networkx(H)

    # Set vertex labels to node names
    H_igraph.vs["label"] = H_igraph.vs["_nx_name"]

    centrality_values = []
    for node in H_igraph.vs["label"]:
      if node in top_nodes:
        centrality_values.append(centrality[node])

    centrality_values = np.array(centrality_values)


    # Scale centrality values to determine node sizes
    #centrality_values = np.array([centrality[node] for node in top_nodes])
    scaled_centrality = (np.array(centrality_values) - min(centrality_values)) / (max(centrality_values) - min(centrality_values))

    # Assign scaled sizes to vertices
    H_igraph.vs["size"] = scaled_centrality*150

    # Assign colors based on centrality values using a colormap
    cmap = plt.cm.plasma # You can choose other colormaps like 'plasma', 'inferno', 'magma'
    norm = plt.Normalize(vmin=min(centrality_values), vmax=max(centrality_values))
    colors = [cmap(norm(value)) for value in centrality_values]
    colors_hex = [mcolors.rgb2hex(c[:3]) for c in colors]  # Convert to hex colors

    for idx, vertex in enumerate(H_igraph.vs):
        vertex["color"] = colors_hex[idx]

    # Prepare visual style
    visual_style = {}
    visual_style["vertex_shape"] = "circle"
    visual_style["vertex_color"] = H_igraph.vs["color"]
    visual_style["vertex_label_dist"] = 0  # Adjust label distance from nodes
    visual_style["edge_color"] = "black"
    visual_style["bbox"] = (1600, 1600)
    visual_style["margin"] = 120
    visual_style["vertex_size"] = H_igraph.vs["size"]
    visual_style["vertex_label_size"] = 25 # Size of the vertex labels

    # Extract labels of top nodes in the order of centrality

    print("Top node labels:", top_nodes)
    plt.figure(figsize=(25, 25))

    # Plot the graph
    ig.plot(H_igraph, "graph_plot.png", **visual_style)
    image = plt.imread("graph_plot.png")


    plt.title(centrality_measure+" centrality", size= 20)
    plt.imshow(image)
    plt.show()  # Display the plot

# Example usage
# Replace G1 with your actual NetworkX graph object

create_graph(G1, centrality_measure="eigenvector", top_n=20)

senators = ['GOPLeader', 'RepJeffDuncan', 'RepFranklin', 'RepMikeJohnson', 'RepJohnRose', 'RepTomEmmer', 'RepChipRoy', 'RepDonaldsPress', 'RepLisaMcClain', 'RepAndyBiggsAZ', 'RepRosendale', 'RepHartzler', 'repkevinhern', 'RepMMM', 'RepKatCammack', 'CongressmanHice', 'RepDonBeyer', 'RepTroyNehls', 'RepBuddyCarter', 'RepDanBishop']
parties = []
for s in senators : parties.append((s,party_affiliations_dict[s]))
parties

create_graph(G1, centrality_measure="closeness", top_n=20)

senators = ['GOPLeader', 'RepDonBeyer', 'RepFranklin', 'RepMMM', 'RepMikeJohnson', 'RepAdams', 'RepDonaldsPress', 'RepBonnie', 'RepTomEmmer', 'RepChipRoy', 'RepTroyNehls', 'RepJeffDuncan', 'RepChuyGarcia', 'RepAndyLevin', 'RepDonBacon', 'LeaderHoyer', 'RepCasten', 'RepKatCammack', 'michaelcburgess', 'RepJoshG']
parties = []
for s in senators : parties.append((s,party_affiliations_dict[s]))
parties

create_graph(G1, centrality_measure="betweenness", top_n=20)

senators = ['GOPLeader', 'SpeakerPelosi', 'LeaderHoyer', 'RepBobbyRush', 'RepMMM', 'RepFranklin', 'JohnCornyn', 'RepAndyLevin', 'RepBonnie', 'RepAndyBiggsAZ', 'RepTomEmmer', 'RepAdams', 'SteveScalise', 'RepStefanik', 'RepChipRoy', 'RepMaloney', 'RepMikeJohnson', 'RepSpanberger', 'RepDonBeyer', 'RepCasten']
parties = []
for s in senators : parties.append((s,party_affiliations_dict[s]))
parties

create_graph(G1, centrality_measure="degree", top_n=20)

senators = ['GOPLeader', 'SpeakerPelosi', 'RepBobbyRush', 'LeaderHoyer', 'RepFranklin', 'RepAndyLevin', 'RepMikeJohnson', 'RepJeffDuncan', 'SteveScalise', 'RepDonBeyer', 'RepCasten', 'RepTomEmmer', 'RepMMM', 'RepAndyBiggsAZ', 'RepDonaldsPress', 'RepChipRoy', 'RepAdams', 'RepBonnie', 'RepJohnRose', 'RepStefanik']
parties = []
for s in senators : parties.append((s,party_affiliations_dict[s]))
parties

create_graph(G1, centrality_measure="hits", top_n=20)

senators = ['GOPLeader', 'RepChipRoy', 'RepMikeJohnson', 'RepAndyBiggsAZ', 'CongressmanHice', 'RepFranklin', 'RepDonaldsPress', 'RepThomasMassie', 'RepTomEmmer', 'RepMMM', 'RepJeffDuncan', 'RepJohnRose', 'Rep_Clyde', 'RepLisaMcClain', 'SteveScalise', 'RepRosendale', 'RepKatCammack', 'RepHerrell', 'RodneyDavis', 'RepBethVanDuyne']
parties = []
for s in senators : parties.append((s,party_affiliations_dict[s]))
parties

create_graph(G1, centrality_measure="hits", top_n=20)

senators = ['RepBobGood', 'RepCloudTX', 'SteveScalise', 'RepJamesComer', 'VernBuchanan', 'replouiegohmert', 'RepBoebert', 'RepTiffany', 'RepRalphNorman', 'GOPLeader', 'RepMaryMiller', 'RepChrisStewart', 'RepRichHudson', 'RepFranklin', 'RepMariaSalazar', 'RepCarlos', 'SenMikeLee', 'RepAndyBiggsAZ', 'RepBost', 'RepTedBudd']
parties = []
for s in senators : parties.append((s,party_affiliations_dict[s]))
parties

create_graph(G1, centrality_measure="pagerank", top_n=20)

senators = ['GOPLeader', 'RepCasten', 'RepChipRoy', 'RepMikeJohnson', 'RepChuyGarcia', 'RepFranklin', 'RepAdams', 'CongressmanHice', 'RepAndyLevin', 'RepAndyBiggsAZ', 'RepThomasMassie', 'RepDonBeyer', 'RepTomEmmer', 'RepMMM', 'RepBonnie', 'RepJayapal', 'RepDonaldsPress', 'LeaderHoyer', 'RepDwightEvans', 'RepJeffDuncan']
parties = []
for s in senators : parties.append((s,party_affiliations_dict[s]))
parties

"""**Viral centrality**"""

# @title Default title text
def viral_centrality(inList, inWeight, outList, Niter = 5, beta = 1.0, tol = 0.0001):
    ''' User has a choice to either run each simulation until the probabilities have converged within
    a specified relative tolerance, or just iterate for 'Niter' iterations for each seed node. If Niter is less than 1, the former option is selected, and
    if Niter is 1 or greater, the latter option is selected.
    inList[i] is list of all the nodes sending connections to i; inWeight[i] is list of corresponding weights (ie transmission probabilities)
    outList[i] is  a list of all the nodes i sends connections to
    All transmission probabilities are universally multiplied by 'beta' '''

    N=len(inList)

    avg_infections = np.zeros(N)

    if Niter < 1: #if Niter is less than 1, this means we want to iterate until the uninfected array has converged to within the prescribed relative tolerance

        for seed in range(N):
            prev_uninfected = np.ones(N)
            uninfected = np.ones(N) #probabilty that node hasn't been infected yet. starts at one for each node
            last_infected = np.zeros(N) #probability that node was infected on last timestep
            cur_infected = np.zeros(N) #probability that node was infected on current timestep
            last_infected[seed] = 1
            uninfected[seed] = 0

            t = 0
            #breadth-first search (BFS)
            bfs_queue = -1 * np.ones(N, dtype=int) #first-in/first-out buffer to perform breadth-first search (see section 10.3.3 of Mark Newman's textbook)
            bfs_queue[0] = seed
            seed_distance = -1 * np.ones(N, dtype=int) #array of distance from seed node
            seed_distance[seed] = 0
            read=0
            write=1
            while np.nanmax( (prev_uninfected[bfs_queue[0:write]] - uninfected[bfs_queue[0:write]]) / prev_uninfected[bfs_queue[0:write]]) > tol: #iterate until relative tolerance is met; do not need absolute value bc. prev_uninfected will always be geq to uninfected; ; note there will always be an 'nan' in entry corresponding to seed node, hence the nanmax function
                prev_uninfected = np.copy(uninfected)

                #expand BFS to find next ring of nodes that are within seed node's reach (see section 10.3.3 of Mark Newman's text)
                if read != write: #if read==write, then breadth-first search is exhausted
                    write_start = write
                    while read < write_start: #when read is equal to write_start, that signifies reaching the end of the "t+1st ring"
                        for neighb in outList[bfs_queue[read]]:
                            if seed_distance[neighb] < 0: #if distance from seed node has not yet been determined, then record it
                                seed_distance[neighb] = t+1
                                bfs_queue[write] = neighb
                                write += 1
                        read += 1


                for node in bfs_queue[0:write]: #for all nodes within reach of the seed node at this point in time
                    prob_uninfected = 1 #set probability of being uninfected to one
                    for con in range(len(inList[node])): #look at each incoming connection to node
                        prob_uninfected = prob_uninfected*(1-(last_infected[inList[node][con]]*(beta*inWeight[node][con])))
                        #prob that node remains uninfected decreases as we go through each connection. beta again multiplies weights to reset max
                    cur_infected[node] = (1-prob_uninfected)*uninfected[node]
                    #prob of current infection is prob of being infected times the prob that node hasn't been infected yet

                for node in bfs_queue[0:write]:
                    last_infected[node] = cur_infected[node]
                    #update our last infected list for each timestep
                    uninfected[node] = uninfected[node] - cur_infected[node]
                    #new prob of being uninfected is old prob - the prob that node is currently infected

                t = t+1

            avg_infections[seed] = sum(1 - uninfected) - 1 #don't want to include seed node infection in total

    else: #if Niter is a positive integer, then just iterate for that number of time steps

        for seed in range(N):
            prev_uninfected = np.ones(N)
            uninfected = np.ones(N) #probabilty that node hasn't been infected yet. starts at one for each node
            last_infected = np.zeros(N) #probability that node was infected on last timestep
            cur_infected = np.zeros(N) #probability that node was infected on current timestep
            last_infected[seed] = 1
            uninfected[seed] = 0

            t = 0
            #breadth-first search (BFS)
            bfs_queue = -1 * np.ones(N, dtype=int) #first-in/first-out buffer to perform breadth-first search (see section 10.3.3 of Newman's textbook)
            bfs_queue[0] = seed
            seed_distance = -1 * np.ones(N, dtype=int) #array of distance from seed node
            seed_distance[seed] = 0
            read=0
            write=1
            while t < Niter: #in contrast to original algorithm, just iterate for fixed number of time steps
                prev_uninfected = np.copy(uninfected)

                #expand BFS to find next ring of nodes that are within seed node's reach (see section 10.3.3 of Newman's text)
                if read != write: #if read==write, then breadth-first search is exhausted
                    write_start = write
                    while read < write_start: #when read is equal to write_start, that signifies reaching the end of the "t+1st ring"
                        for neighb in outList[bfs_queue[read]]:
                            if seed_distance[neighb] < 0: #if distance from seed node has not yet been determined, then record it
                                seed_distance[neighb] = t+1
                                bfs_queue[write] = neighb
                                write += 1
                        read += 1


                for node in bfs_queue[0:write]: #for all nodes within reach of the seed node at this point in time
                    prob_uninfected = 1 #set probability of being uninfected to one
                    for con in range(len(inList[node])): #look at each incoming connection to node
                        prob_uninfected = prob_uninfected*(1-(last_infected[inList[node][con]]*(beta*inWeight[node][con])))
                        #prob that node remains uninfected decreases as we go through each connection. beta again multiplies weights to reset max
                    cur_infected[node] = (1-prob_uninfected)*uninfected[node]
                    #prob of current infection is prob of being infected times the prob that node hasn't been infected yet

                for node in bfs_queue[0:write]:
                    last_infected[node] = cur_infected[node]
                    #update our last infected list for each timestep
                    uninfected[node] = uninfected[node] - cur_infected[node]
                    #new prob of being uninfected is old prob - the prob that node is currently infected

                t = t+1

            avg_infections[seed] = sum(1 - uninfected) - 1 #dont want to include seed node infection in total

    return avg_infections

import numpy as np
import matplotlib.pyplot as plt

# Assuming viral_centrality function and other variables (inList, inWeight, outList, usernameList) are defined
tol = 0.001
num_activated = viral_centrality(inList, inWeight, outList, Niter = -1, tol = tol)

plt.figure(figsize=(12, 8))

# Plot scatter
sc = plt.scatter(np.array(range(len(num_activated))), num_activated, label='Viral Centrality', cmap="plasma")

# Add horizontal line at y=0.7
plt.axhline(y=0.7, color='r', linestyle='--', label='Threshold 0.7')

senators = []
# Annotate with labels for nodes with activations greater than 0.6
for i, num in enumerate(num_activated):
    if num > 0.6:
        plt.text(i, num + 0.02, f'{usernameList[i],(party_affiliations_dict[usernameList[i]])}', fontsize=9, ha='center', va='bottom')
        senators.append((usernameList[i],party_affiliations_dict[usernameList[i]]))

# Add labels and title
plt.xlabel('Node ID', fontsize=15)
plt.ylabel('Avg Number Activated', fontsize=15)
plt.title('Viral Centrality Activation', fontsize=18)

# Add legend
plt.legend()

# Display plot
plt.show()
print(senators)

import seaborn as sns
plt.figure(figsize=(12, 6))
sns.boxplot(x=party_affiliations, y=num_activated)
plt.title('Distribution of Viral Centrality by Party')
plt.xlabel('Party')
plt.ylabel('Viral Centrality')
plt.show()

democrats = [num_activated[i] for i in range(0, len(num_activated)) if party_affiliations[i]=="Democratic"]
republicans = [num_activated[i] for i in range(0, len(num_activated)) if party_affiliations[i]=="Republican"]

t_stat, p_value = ttest_ind(democrats, republicans)

print(f"T-test results: t-statistic = {t_stat}, p-value = {p_value}")

if p_value < 0.05:
    print("The difference in viral centrality between Democrats and Republicans is statistically significant.")
else:
    print("The difference in viral centrality between Democrats and Republicans is not statistically significant.")

def top_k_nodes(num_activated, k):

  plt.style.use('seaborn-v0_8-colorblind')
  top_k_indices = np.argsort(num_activated)[-k:][::-1]

  top_k_scores = num_activated[top_k_indices]

  top_k_names = [usernameList[i] for i in top_k_indices]
  top_k_parties = [party_affiliations[i] for i in top_k_indices]
  label_counts = Counter(top_k_parties)
  total_labels = sum(label_counts.values())

  # Calculate the percentages
  percentages = {label: (count / total_labels) * 100 for label, count in label_counts.items()}

  # Plotting the percentages
  party_labels = list(percentages.keys())
  sizes = list(percentages.values())
  colors = ['blue', 'red', 'green']  # Assuming blue for Democratic, red for Republican and green for Independent

  # Plot a pie chart
  plt.figure(figsize=(8, 8))
  plt.pie(sizes, labels=party_labels, autopct='%1.1f%%', startangle=140, colors=colors)
  plt.title(f"Parties percentages in the top {k} nodes")
  plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.
  plt.show()

top_k_nodes(num_activated, 10)

from matplotlib import style
plt.style.use('seaborn-v0_8-colorblind')
all_weights = []
for single_node_weights in outWeight:
    all_weights.extend(single_node_weights)

n, bins, patches = plt.hist(all_weights, bins=100, density=True)

# best fit of data for lognorm distribution
s, loc, scale=lognorm.fit(all_weights,floc=0.0)

x=np.linspace(0,0.14,10000)
y=lognorm.pdf(x,s, loc=loc, scale=scale)
plt.plot(x,y,label='lognorm',linewidth=4)

plt.legend()

from sklearn.preprocessing import MinMaxScaler
# Compute eigenvector centrality
centrality = nx.eigenvector_centrality(G1)

# Use the Louvain communities list
# Partition list should have community membership for each node
## Replace with actual partitions

# Convert NetworkX graph to igraph
g = ig.Graph.from_networkx(G1)
g.vs["label"] = g.vs["_nx_name"]

# Prepare data for visualization
node_names = list(G1.nodes())
node_sizes = np.array([centrality[node] for node in node_names])  # Get centrality values

# Normalize centrality values to use as label sizes
scaler = MinMaxScaler(feature_range=(10, 30))  # Adjust the range as needed
label_sizes = scaler.fit_transform(node_sizes.reshape(-1, 1)).flatten()

# Identify the top 20 nodes with highest centrality values
top_20_indices = np.argsort(node_sizes)[-20:]
top_20_nodes = [node_names[idx] for idx in top_20_indices]
partition=[louvain_filtered[idx] for idx in top_20_indices]

# Create a subgraph containing only the top 20 nodes
G1_top_20 = G1.subgraph(top_20_nodes)
g_top_20 = ig.Graph.from_networkx(G1_top_20)
#G2_top_20.vs["label"] = G2_top_20.vs["name"]

# Assign colors to communities for top 20 nodes
community_colors = {
    1: 'blue',
    2: 'red',
    3: 'green',
    4: 'Gray'
}

# Colors for top 20 nodes
colors = [community_colors[node] for node in partition]

# Update label sizes for top 20 nodes
top_20_label_sizes = [label_sizes[idx] for idx in top_20_indices]
# Create visual style dictionary
visual_style = {}
#node size
visual_style["vertex_size"] = 10000*top_20_label_sizes
#node color
visual_style["vertex_label_color"] = colors
#node label
visual_style["vertex_label"] = g.vs["label"]
#node shape
visual_style["vertex_shape"] = "hidden"
#label size
visual_style["vertex_label_size"] = 10000*top_20_label_sizes
#edge color
visual_style["edge_color"] = visual_style["vertex_label_color"]
#edge thickness
visual_style["edge_width"] = 0
#bounding box
visual_style["bbox"] = (1024, 1024)
#margin
visual_style["margin"] = 10
#layout
visual_style["layout"] = "kk"
#background
visual_style["background"] = "white"

# Plot the graph
ig.plot(g_top_20, **visual_style)

"""# **Measuring the centrality for the isolated nodes**"""

import networkx as nx

# Function to compute centrality measures for specific nodes
def compute_centrality(G, isolated_nodes, centrality_measure=None):

    # Compute the specified centrality measure
    if centrality_measure == "eigenvector":
        centrality = nx.eigenvector_centrality(G)
    elif centrality_measure == "closeness":
        centrality = nx.closeness_centrality(G)
    elif centrality_measure == "betweenness":
        centrality = nx.betweenness_centrality(G)
    elif centrality_measure == "degree":
        centrality = nx.degree_centrality(G)
    elif centrality_measure == "hits":
        hits = nx.hits(G)
        centrality = hits[0]
    elif centrality_measure == "pagerank":
        centrality = nx.pagerank(G)
    else:
        raise ValueError("Invalid centrality measure. Choose from 'eigenvector', 'closeness', 'betweenness', or 'degree'.")

    # Extract centrality values for the given nodes
    centrality_values = {node: centrality.get(node, 0) for nodes in isolated_nodes for node in nodes}

    return centrality_values

# Example usage
IS1 = ['RepCindyAxne', 'SanfordBishop', 'RepChrisPappas']
IS2 = ['RepLizCheney',
       'RepJohnCurtis',
       'RepKayGranger',
       'RepElaineLuria',
       'RepOHalleran',
       'RepChrisPappas',
       'RepTomSuozzi',
       'RepAnnWagner',
       'RepJoeWilson',
       'RobWittman']
IS3 = ['SenDanSullivan',
       'RepEdCase',
       'RepRonEstes',
       'RepKayGranger',
       'RepJoshHarder',
       'RepKirkpatrick',
       'RepAlLawsonJr',
       'RepOHalleran',
       'Kilili_Sablan',
       'RepSchrader',
       'CongMikeSimpson',
       'RepTomSuozzi']

# Compute degree centrality for IS1 nodes
centrality_measure = "eigenvector"
centrality_values_IS1 = compute_centrality(G1,[IS1], centrality_measure)
print(f"{centrality_measure} centrality values for IS1 nodes:")
print(centrality_values_IS1)
# Compute degree centrality for IS1 nodes
centrality_measure = "degree"
centrality_values_IS1 = compute_centrality(G1,[IS1], centrality_measure)
print(f"{centrality_measure} centrality values for IS1 nodes:")
print(centrality_values_IS1)
# Compute closeness centrality for IS1 nodes
centrality_measure = "closeness"
centrality_values_IS1 = compute_centrality(G1,[IS1], centrality_measure)
print(f"{centrality_measure} centrality values for IS2 nodes:")
print(centrality_values_IS1)
# Compute betweenness centrality for IS1 nodes
centrality_measure = "betweenness"
centrality_values_IS1 = compute_centrality(G1,[IS1], centrality_measure)
print(f"{centrality_measure} centrality values for IS3 nodes:")
print(centrality_values_IS1)
# Compute pagerank centrality for IS1 nodes
centrality_measure = "pagerank"
centrality_values_IS1 = compute_centrality(G1,[IS1], centrality_measure)
print(f"{centrality_measure} centrality values for IS3 nodes:")
print(centrality_values_IS1)
# Compute hits centrality for IS1 nodes
centrality_measure = "hits"
centrality_values_IS1 = compute_centrality(G1,[IS1], centrality_measure)
print(f"{centrality_measure} centrality values for IS3 nodes:")
print(centrality_values_IS1)


print("---------------------------------------")


# Compute degree centrality for IS1 nodes
centrality_measure = "eigenvector"
centrality_values_IS1 = compute_centrality(G1,[IS2], centrality_measure)
print(f"{centrality_measure} centrality values for IS1 nodes:")
print(centrality_values_IS1)
# Compute degree centrality for IS2 nodes
centrality_measure = "degree"
centrality_values_IS1 = compute_centrality(G1,[IS2], centrality_measure)
print(f"{centrality_measure} centrality values for IS1 nodes:")
print(centrality_values_IS1)
# Compute closeness centrality for IS2 nodes
centrality_measure = "closeness"
centrality_values_IS2 = compute_centrality(G1,[IS2], centrality_measure)
print(f"{centrality_measure} centrality values for IS2 nodes:")
print(centrality_values_IS2)
# Compute betweenness centrality for IS2 nodes
centrality_measure = "betweenness"
centrality_values_IS3 = compute_centrality(G1,[IS2], centrality_measure)
print(f"{centrality_measure} centrality values for IS3 nodes:")
print(centrality_values_IS3)
# Compute betweenness centrality for IS2 nodes
centrality_measure = "betweenness"
centrality_values_IS3 = compute_centrality(G1,[IS2], centrality_measure)
print(f"{centrality_measure} centrality values for IS3 nodes:")
print(centrality_values_IS3)
# Compute pagerank centrality for IS2 nodes
centrality_measure = "pagerank"
centrality_values_IS1 = compute_centrality(G1,[IS2], centrality_measure)
print(f"{centrality_measure} centrality values for IS3 nodes:")
print(centrality_values_IS1)
# Compute hits centrality for IS2 nodes
centrality_measure = "hits"
centrality_values_IS1 = compute_centrality(G1,[IS2], centrality_measure)
print(f"{centrality_measure} centrality values for IS3 nodes:")
print(centrality_values_IS1)



print("---------------------------------------")

# Compute degree centrality for IS1 nodes
centrality_measure = "eigenvector"
centrality_values_IS1 = compute_centrality(G1,[IS3], centrality_measure)
print(f"{centrality_measure} centrality values for IS1 nodes:")
print(centrality_values_IS1)
# Compute degree centrality for IS1 nodes
centrality_measure = "degree"
centrality_values_IS1 = compute_centrality(G1,[IS3], centrality_measure)
print(f"{centrality_measure} centrality values for IS1 nodes:")
print(centrality_values_IS1)
# Compute closeness centrality for IS1 nodes
centrality_measure = "closeness"
centrality_values_IS2 = compute_centrality(G1,[IS3], centrality_measure)
print(f"{centrality_measure} centrality values for IS2 nodes:")
print(centrality_values_IS2)
# Compute betweenness centrality for IS3 nodes
centrality_measure = "betweenness"
centrality_values_IS3 = compute_centrality(G1,[IS3], centrality_measure)
print(f"{centrality_measure} centrality values for IS3 nodes:")
print(centrality_values_IS3)
# Compute betweenness centrality for IS3 nodes
centrality_measure = "betweenness"
centrality_values_IS3 = compute_centrality(G1,[IS3], centrality_measure)
print(f"{centrality_measure} centrality values for IS3 nodes:")
print(centrality_values_IS3)
# Compute pagerank centrality for IS2 nodes
centrality_measure = "pagerank"
centrality_values_IS1 = compute_centrality(G1,[IS3], centrality_measure)
print(f"{centrality_measure} centrality values for IS3 nodes:")
print(centrality_values_IS1)
# Compute hits centrality for IS2 nodes
centrality_measure = "hits"
centrality_values_IS1 = compute_centrality(G1,[IS3], centrality_measure)
print(f"{centrality_measure} centrality values for IS3 nodes:")
print(centrality_values_IS1)

"""# **HITS (Authority)**"""

# Compute hits centrality for IS1 nodes
centrality_measure = "hits"
centrality_values_IS1 = compute_centrality(G1,[IS1], centrality_measure)
print(f"{centrality_measure} centrality values for IS3 nodes:")
print(centrality_values_IS1)

centrality_measure = "hits"
centrality_values_IS1 = compute_centrality(G1,[IS2], centrality_measure)
print(f"{centrality_measure} centrality values for IS3 nodes:")
print(centrality_values_IS1)

centrality_measure = "hits"
centrality_values_IS1 = compute_centrality(G1,[IS3], centrality_measure)
print(f"{centrality_measure} centrality values for IS3 nodes:")
print(centrality_values_IS1)

import matplotlib.pyplot as plt

# Example data for IS1, IS2, IS3
# Replace with your actual data
nodes_IS1 = range(3)  # Assuming 12 nodes in IS1
nodes_IS2 = range(3, 13)  # Next 12 nodes in IS2
nodes_IS3 = range(13, 25)  # Last 12 nodes in IS3

# Centrality values (sample data, replace with actual values)
eigenvector_IS1 = [0.010172195737560584, 0.00811907629184256, 0.007704148574527629]
degree_IS1 = [0.05485232067510548, 0.02953586497890295, 0.05274261603375527]
closeness_IS1 = [0.3907666941467436, 0.3767885532591415, 0.3838056680161943]
betweenness_IS1 = [0.0007356622084069054, 0.00015906725918858716, 0.0006640548014461723]
pagerank_IS1 = [0.0008313589141990084, 0.0005082537724664958, 0.0012706616291607302]
hits_IS1 = [0.0005264808517320701, 0.00010132345965276854, 0.0006796786968566949]

eigenvector_IS2 = [0.005903843447792194, 0.016896544735108363, 0.002070272501717947, 0.012049332402056852, 6.699797681052669e-77,\
                   0.007704148574527629, 6.699797681052669e-77, 6.699797681052669e-77, 0.028875964395108082, 0.012920184134237707]
degree_IS2 = [0.04641350210970464, 0.0780590717299578, 0.014767932489451475, 0.08016877637130801, 0.02109704641350211, \
              0.05274261603375527, 0.010548523206751054, 0.05274261603375527, 0.12447257383966244, 0.06751054852320675]
closeness_IS2 = [0.38072289156626504, 0.43726937269372695, 0.3454810495626822, 0.41798941798941797, 0.0, 0.3838056680161943\
                 , 0.0, 0.0, 0.4570877531340405, 0.4111014744145707]
betweenness_IS2 = [0.00031850855910959804, 0.002023373426757558, 4.7417393998207524e-05, 0.0022458663355766242,\
                   0.0, 0.0006640548014461723, 0.0, 0.0, 0.004515517259357964, 0.0015808561121694383]
pagerank_IS2 = [0.0005295017215365617, 0.0009928285719296038, 0.0003988479526175712, 0.0018476344093673715,\
                0.0003157894736842106, 0.0012706616291607302, 0.0003157894736842106, 0.0003157894736842106, 0.0017724520237509841, 0.0009275942581819601]
hits_IS2 = [0.0003341609648157106, 0.00131483014603435, 6.222420083602712e-05, 0.0008399539606651152, 6.524780011460216e-19, \
            0.000679678696856697, -1.3049560022920432e-18, 9.787170017190323e-19, 0.001135098693527861, 0.0004080480247846492]

eigenvector_IS3 = [0.004095959371772002, 0.0028098552722667933, 0.00155320668249774, 0.002070272501717947, 6.699797681052669e-77, 0.007012468006470299,\
                   0.004133733846837748, 6.699797681052669e-77, 6.162330617573702e-05, 0.005537837642795757, 0.006632545083676061, 6.699797681052669e-77]
degree_IS3 = [0.044303797468354424, 0.02320675105485232, 0.018987341772151896, 0.014767932489451475, 0.010548523206751054, 0.02320675105485232,\
              0.02320675105485232, 0.02109704641350211, 0.004219409282700422, 0.016877637130801686, 0.02109704641350211, 0.010548523206751054]
closeness_IS3 = [0.36944660950896335, 0.3487858719646799, 0.32266848196051734, 0.3454810495626822, 0.0, 0.38041733547351525,\
                 0.34981549815498153, 0.0, 0.24496124031007752, 0.36715724244771497, 0.3744075829383886, 0.0]
betweenness_IS3 = [0.000657791702666593, 0.00013842609968978204, 3.2595114313752626e-05, 4.7417393998207524e-05,\
                   0.0, 7.281032458550411e-05, 3.742271847094731e-05, 0.0, 1.1954929191747383e-06, 8.873956890566535e-05, 7.419782931665222e-05, 0.0]
pagerank_IS3 = [0.0007163501920365283, 0.0005411455409242549, 0.0003345042805472155, 0.0003988479526175712, 0.0003157894736842106, 0.0005941670445826241\
                , 0.0004353763755116233, 0.0003157894736842106, 0.0003270623092514263, 0.0006342243641930573, 0.0004928497453782596, 0.0003157894736842106]
hits_IS3 = [0.000577325627574701, 7.108429649058733e-05, 4.896739909945982e-05, 6.222420083602575e-05, 1.3049560022920426e-18, 9.329691136066956e-05,\
            0.00022152952434061816, 9.78717001719032e-19, 7.556650234452323e-05, 0.00015218205450668023, 0.00015063542391502408, 1.3049560022920426e-18]

# Plotting
plt.title("Isolated Nodes after applying different community detection methos (Louvain, Infomap, Girvan-Newman)")

plt.figure(figsize=(18, 12))

plt.subplot(231)
plt.scatter(nodes_IS1, eigenvector_IS1, color='blue', label='IS1')
plt.scatter(nodes_IS2, eigenvector_IS2, color='green', label='IS2')
plt.scatter(nodes_IS3, eigenvector_IS3, color='red', label='IS3')
plt.title('Eigenvector Centrality')
plt.xlabel('Nodes')
plt.ylabel('Values')
plt.ylim(0, 0.16)
plt.legend()

plt.subplot(232)
plt.scatter(nodes_IS1, degree_IS1, color='blue', label='IS1')
plt.scatter(nodes_IS2, degree_IS2, color='green', label='IS2')
plt.scatter(nodes_IS3, degree_IS3, color='red', label='IS3')
plt.title('Degree Centrality')
plt.xlabel('Nodes')
plt.ylabel('Values')
plt.ylim(0, 0.5)
plt.legend()

plt.subplot(233)
plt.scatter(nodes_IS1, closeness_IS1, color='blue', label='IS1')
plt.scatter(nodes_IS2, closeness_IS2, color='green', label='IS2')
plt.scatter(nodes_IS3, closeness_IS3, color='red', label='IS3')
plt.title('Closeness Centrality')
plt.xlabel('Nodes')
plt.ylabel('Values')
plt.ylim(0, 0.5)
plt.legend()

plt.subplot(234)
plt.scatter(nodes_IS1, betweenness_IS1, color='blue', label='IS1')
plt.scatter(nodes_IS2, betweenness_IS2, color='green', label='IS2')
plt.scatter(nodes_IS3, betweenness_IS3, color='red', label='IS3')
plt.title('Betweenness Centrality')
plt.xlabel('Nodes')
plt.ylabel('Values')
plt.ylim(0,0.07)
plt.legend()

plt.subplot(235)
plt.scatter(nodes_IS1, pagerank_IS1, color='blue', label='IS1')
plt.scatter(nodes_IS2, pagerank_IS2, color='green', label='IS2')
plt.scatter(nodes_IS3, pagerank_IS3, color='red', label='IS3')
plt.title('PageRank Centrality')
plt.xlabel('Nodes')
plt.ylabel('Values')
plt.ylim(0,0.016)
plt.legend()

plt.subplot(236)
plt.scatter(nodes_IS1, hits_IS1, color='blue', label='IS1')
plt.scatter(nodes_IS2, hits_IS2, color='green', label='IS2')
plt.scatter(nodes_IS3, hits_IS3, color='red', label='IS3')
plt.title('HITS Centrality')
plt.xlabel('Nodes')
plt.ylabel('Values')
plt.ylim(0,0.02)
plt.legend()

plt.tight_layout()
plt.show()